[{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Backup Amazon Elastic Kubernetes Service (EKS) resources using NetApp Trident Protect Kubernetes is an open source container orchestration platform that automates the deployment, scaling, and management of containerized applications. For users running applications and workloads on Kubernetes, protecting resources and data from accidental deletion or hardware failures is crucial for maintaining business continuity and meeting compliance requirements. While Kubernetes provides high availability through its control plane and worker node redundancy, it does not inherently protect against human errors, such as accidental deletion of namespaces, deployments, or persistent volumes, nor does it safeguard against regional failures or data corruption.\nThe complexity of modern microservices architectures and the increasing scale of Kubernetes deployments make it even more critical to maintain regular, tested backups that can be restored consistently across different environments and Amazon Web Services (AWS) Regions. This includes backing up essential components such as entire namespaces, persistent volumes containing application data, custom resources, and configuration objects. Without proper backup mechanisms, organizations risk extended outages, data loss, and potential breach of service level agreements (SLAs). These can result in significant financial impact and damage to customer trust. This post introduces a new data protection tool and provides a step-by-step guide to implementing a proof-of-concept within a Kubernetes environment.\nNetApp Trident Protect NetApp Trident Protect is a free tool from NetApp that provides Kubernetes cluster data protection, data migration, disaster recovery, and movement of containerized workloads across AWS Regions and on-premises environments. It enables on-demand or scheduled data protection tasks for Kubernetes cluster resources and persistent volumes to externally supported storage backends such as Amazon S3. It offers automation capabilities through its Kubernetes-native API and powerful tridentctl-protect CLI, enabling programmatic access for seamless integration with existing workflows. AWS users can use Trident Protect to handle operations such as cross-cluster disaster recovery (between AWS Regions or within a Region), migration of stateful services between storage services, or moving resources running on a self-managed Kubernetes cluster into Amazon Elastic Kubernetes Service (Amazon EKS). The Amazon EKS and Trident backup architecture is shown in Figure 1.\nFigure 1. Amazon EKS and Trident backup architecture.\nAmazon FSx for NetApp ONTAP file system is a fully managed shared storage service, built on NetApp’s popular ONTAP file system. It provides users access to ONTAP’s enterprise storage features and services, such as thin provisioning, data deduplication, and the Snapshot functionality that drives NetApp agility in storage provisioning and data protection.\nIn this post, we focus on using Trident Protect with our AWS retail sample store application to handle data protection and migration tasks running on an EKS cluster. We also dive deep into some backup, recovery, and storage migration options that help you decide what is best to implement in your organization’s environment. The following architecture creates backups in our local AWS Region, which can be used to restore other namespaces within the same cluster and migrate from different AWS storage services.\nSoftware architecture The AWS retail store sample application is built from microservices and backed by several stateful services. In total they consume four Persistent Volume Claims (PVC) for each stack as in Figure 2.\nAssets: Serves static assets such as images related to the product catalog—needs RWX volume such as NFS. Orders: Receive and process user orders backed by MySQL DB and RabbitMQ—needs two RWO block storage volumes. Catalog: Product listings and details backed by MySQL DB—needs RWO block storage volume. Figure 2. Sample retail store application.\nPrerequisites To manually provision the resources used in this walkthrough, the following list provides the components and versions used. Alternatively, use the provided Terraform resources to automate the entire deployment as shown in Step 1 of the walkthrough.\nAmazon EKS cluster deployed version 1.32 Pod Identity (eks-pod-identity-agent) add-on version v1.3.4 NetApp Trident CSI add-on version 25.06 Amazon EBS CSI add-on version v1.40.0-eksbuild.1 Snapshot controller (snapshot-controller) add-on version v8.0.0 Create VolumeSnapshotClass for Amazon Elastic Block Storage (Amazon EBS) and FSx for ONTAP (sample manifests are provisioned as part of the terraform deployment). Load balancer controller (aws-load-balancer-controller) version 2.11.0 Deployed FSx for ONTAP file system. Walkthrough Create necessary infrastructure with Terraform Clone the sample repository from GitHub and create all relevant resources using the Terraform code in the repository:\ngit clone https://github.com/aws-samples/sample-eks-backup-with-trident-protect.git cd sample-eks-backup-with-trident-protect/terraform terraform init terraform apply -auto-approve NOTE: The Terraform script prompts you to input a public IP address. This is the host’s address that accesses the UI of the sample application.\nvar.ui_service_public_ip The public IP addess of the host that will access the sample application \u0026gt;UI from the web browser Enter a value: A.B.C.D/32 This deployment can take 20-25 minutes to complete. When finished, the output of the command should look like the following:\nfsx-management-ip = toset([ \u0026#34;10.0.1.13\u0026#34;, ]) fsx-ontap-id = \u0026#34;fs-a1b2c3d4e5f6g7h8i\u0026#34; fsx-svm-name = \u0026#34;ekssvm\u0026#34; region = \u0026#34;us-east-1\u0026#34; secret_arn = \u0026#34;arn:aws:secretsmanager:us-east-1:0123456789ab:secret:fsxn-password-secret-8DKLpwTi-8DLlwE\u0026#34; zz_update_kubeconfig_command = \u0026#34;aws eks update-kubeconfig --name eks-protect-8DKLpwTi --alias eks-primary --region us-east-1\u0026#34; Next, copy and run the AWS Command Line Interface (AWS CLI) command from the update_kubeconfig_command output and check that you can reach the cluster by running kubectl get nodes:\nNAME STATUS ROLES AGE VERSION ip-10-0-1-120.ec2.internal Ready \u0026lt;none\u0026gt; 2m15s v1.32.1-eks-5d632ec ip-10-0-1-125.ec2.internal Ready \u0026lt;none\u0026gt; 95m v1.32.1-eks-5d632ec ip-10-0-2-42.ec2.internal Ready \u0026lt;none\u0026gt; 95m v1.32.1-eks-5d632ec ip-10-0-2-95.ec2.internal Ready \u0026lt;none\u0026gt; 2m9s v1.32.1-eks-5d632ec Deploy Trident Protect into EKS cluster Run the following commands to install the Trident Protect:\nIMPORTANT: Make sure that you change clusterName to the EKS cluster name in your environment.\nhelm repo add netapp-trident-protect https://netapp.github.io/trident-protect-helm-chart helm install trident-protect netapp-trident-protect/trident-protect --set clusterName=eks-protect-ymd8hvHr --version 100.2506.0 --create-namespace --namespace trident-protect Expected output:\n\u0026#34;netapp-trident-protect\u0026#34; has been added to your repositories NAME: trident-protect LAST DEPLOYED: Tue Jul 22 19:44:30 2025 NAMESPACE: trident-protect STATUS: deployed REVISION: 1 TEST SUITE: None Create S3 bucket If you have an existing S3 bucket, then you can use it. If not, then use the following steps to create a new S3 bucket. Replace \u0026lt;bucket_name\u0026gt; and \u0026lt;aws_region\u0026gt; with your values:\naws s3 mb s3://\u0026lt;bucket_name\u0026gt; --region \u0026lt;aws_region\u0026gt; Create an EKS secret to store user credentials – Optional EKS Pod Identity is the recommended method for IAM authentication, which has been configured as part of the initial terraform deployments. If you don’t want to use EKS Pod Identity for IAM authentication then create a secret to store the trident protect user AWS accessKey and secretKey. Make sure that the user credentials you provide have the necessary permissions to access the S3 bucket—go to the example Amazon S3 policy statement.\nUse the following example to create the secret:\nkubectl create secret generic \u0026lt;secret-name\u0026gt; \\ --from-literal=accessKeyID=\u0026lt;accessKey\u0026gt; \\ --from-literal=secretAccessKey=\u0026lt;seceretKey\u0026gt; \\ -n trident-protect Create Trident Protect AppVault Create the Trident Protect AppVault. The AppVault points to the S3 bucket where both snapshots and backup content, data and metadata is stored. The AppVault is created in the dedicated trident-protect backup namespace created in Step 2 and can be secured with role-based access control (RBAC) to restrict access to privileged objects to administrators.\nAll remaining tasks are created either in the original application namespace or the target one in case of the restore examples. To create the AppVault, use the protect-vault.yaml sample manifest.\ncd \u0026lt;repo\u0026gt;/manifests Update the following parameters:\nproviderConfig.s3.bucketName: the S3 bucket name providerConfig.s3.endpoint: the S3 endpoint if the bucket is not in the us-east-1 Region useIAM: Use EKS Pod Identity for IAM authentication providerCredentials.accessKeyID.name: the EKS secret name from the previous step providerCredentials.secretAccessKey.name: the EKS secret name from the previous step IMPORTANT: When using useIAM: true with EKS Pod Identity don’t set providerCredentials.accessKeyID.name and providerCredentials.secretAccessKey.name apiVersion: protect.trident.netapp.io/v1 kind: AppVault metadata: name: amazon-s3-trident-protect-src-bucket namespace: trident-protect spec: properties: dataMoverPasswordSecretRef: my-optional-data-mover-secret providerType: AWS providerConfig: s3: bucketName: trident-protect-blog endpoint: s3.amazonaws.com useIAM: true # providerCredentials: # accessKeyID: # valueFromSecret: # key: accessKeyID # name: s3-secret # secretAccessKey: # valueFromSecret: # key: secretAccessKey # name: s3-secret Run the following command to create the AppVault:\nkubectl create -f protect-vault.yaml To check if the AppVault was created successfully, run the following command:\nkubectl get appvault -n trident-protect Expected output:\nNAME STATE ERROR MESSAGE AGE eks-protect-vault Available 4s Create Trident Protect Application To perform data protection operations on your Amazon EKS applications, you need to create a Trident Protect Application resource. An application can be defined in the following ways:\nAs a namespace whereby everything that is part of the namespace must be protected As a subset of a namespace based on labels if you only want to protect part of a namespace (for example, only the PVC) It can span across multiple namespaces An application can also take into account cluster-wide resources To define the namespaces in which the application resources exist, use spec.includedNamespaces and specify namespace labels or a namespace name. You can use this sample trident-application.yaml manifest to define an Application for the sample-app. apiVersion: protect.trident.netapp.io/v1 kind: Application metadata: name: sample-app namespace: tenant0 spec: includedNamespaces: - namespace: tenant0 Run the following to create the Application:\ncd \u0026lt;repo\u0026gt;/manifests kubectl create -f trident-application.yaml To check that the application was created successfully run the following command:\nkubectl get application -n tenant0 Expected output:\nNAME PROTECTION STATE AGE sample-app None 14s Backup, restore, and migration walkthrough Create application backup\nTrident Protect has several data protection options available:\nOn-demand snapshot On-demand backup Data protection schedule Application replication Application migration In this post, we focus on on-demand backup and application migration, but you can read more about how to use the other data protection options in the preceding link references.\nTo create an on-demand backup for the sample application, you create a backup resource for the Application you just created and point it to the Amazon S3 created AppVault. You can use the following trident-backup.yaml sample manifest:\napiVersion: protect.trident.netapp.io/v1 kind: Backup metadata: namespace: tenant0 name: sample-app-backup-1 spec: applicationRef: sample-app appVaultRef: eks-protect-vault Run the following to create the backup:\ncd \u0026lt;repo\u0026gt;/manifests kubectl create -f trident-backup.yaml To check if the Backup was created successfully, run the following command:\nkubectl get backup -n tenant0 Expected output:\nNAME APP RECLAIM POLICY STATE ERROR AGE sample-app-backup-1 sample-app Retain Completed 9m33s IMPORTANT: The state of the backup can be running while the backup is in progress. Wait until the status gets to Completed. If the status is Failed, then use kubectl describe backup -n tenant0 sample-app-backup-1 to get more details on the failure.\nIf you check your Application Protection State, then it is now set to Partial because you just have an on-demand backup set for your application, but no backup schedule. You can use kubectl describe application -n tenant0 to check the status.\nRestore application from a backup\nTrident Protect has several recovery options available:\nRestore from backup Restore from a backup to a different namespace Restore from a backup to the original namespace Restore from a backup to a different cluster Restore from snapshot Restore from a snapshot to a different namespace Restore from a snapshot to the original namespace In this post, we focus on restoring the sample-app backup to a different namespace on the same EKS cluster. You can read more about how to use the other recovery options in the preceding link references. The following is a review of the BackupRestore resources spec:\nspec.appArchivePath: The path in the AppVault where the backup contents are stored. You can retrieve the Archive Path by issuing the following command on your backup: kubectl get backup sample-app-backup-1 -n tenant0 -o jsonpath=\u0026#39;{.status.appArchivePath}\u0026#39; spec.appVaultRef: The name of the AppVault where the backup contents are stored. spec.namespaceMapping: The mapping of the source namespace of the restore operation to the destination namespace. You can use the following trident-protect-backup-restore.yaml sample manifest: apiVersion: protect.trident.netapp.io/v1 kind: BackupRestore metadata: name: sample-app-restore-1 namespace: tenant1 spec: appArchivePath: \u0026lt;APP ARCHIVE PATH\u0026gt; appVaultRef: eks-protect-vault namespaceMapping: - source: tenant0 destination: tenant1 resourceFilter: resourceSelectionCriteria: \u0026#34;Exclude\u0026#34; resourceMatchers: - kind: TargetGroupBinding If you need to choose only specific resources within the application to restore, then use the following filtering to include or exclude resources marked with particular labels.\nresourceFilter.resourceSelectionCriteria: Use Include or Exclude to include or exclude a resource defined in resourceMatchers. resourceFilter.resourceMatchers: resourceMatchers[].group: Group of the resource to be filtered resourceMatchers[].kind: Kind of the resource to be filtered resourceMatchers[].version: Version of the resource to be filtered resourceMatchers[].names: Names in the Kubernetes metadata.name field of the resource to be filtered resourceMatchers[].namespaces: Namespaces in the Kubernetes metadata.name field of the resource to be filtered resourceMatchers[].labelSelectors: Label selector string in the Kubernetes metadata.name field of the resource For an example, you could use this sample code to include resources in your BackupRestore manifest:\nspec: resourceFilter: resourceSelectionCriteria: \u0026#34;Include\u0026#34; resourceMatchers: - group: my-resource-group-1 kind: my-resource-kind-1 version: my-resource-version-1 names: [\u0026#34;my-resource-names\u0026#34;] namespaces: [\u0026#34;my-resource-namespaces\u0026#34;] labelSelectors: [\u0026#34;trident.netapp.io/os=linux\u0026#34;] NOTE: The application recovery uses a LoadBalancer Type service in Amazon EKS, thus you should exclude the TargetGroupBinding resources so that the AWS Load Balancer Controller can create a new TargetGroupBinding for the new namespace that isn’t in conflict with the existing application on the cluster.\nRun the following to create the BackupRestore:\ncd \u0026lt;repo\u0026gt;/manifestskubectl create -f trident-protect-backup-restore.yaml To check if the BackupRestore was created successfully, run the following command:\nkubectl get backuprestore -n tenant1 Expected output:\nNAME STATE ERROR AGE sample-app-restore-1 Completed 115s IMPORTANT: The state of the BackupRestore can be running while the restore is in progress. Wait until the status gets to Completed. If the status is Failed, then use kubectl describe backuprestore -n tenant1 sample-app-restore-1 to get more details on the failure.\nAfter the restore process is finished, you can verify that your recovered application is up and running by accessing the UI service endpoint in your browser. You can get the UI service endpoint by issuing the following command:\nkubectl get svc ui -n tenant1 --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; If everything was successful, then you should get the UI that looks like Figure 3:\n![alt text](\nHình 3. Restored retail store application.\nMigrate between storage services On this step, you migrate parts of the sample application stateful services from one storage service to another. You should do that using the storageClassMapping feature of the Trident Protect BackupRestore. You should migrate the catalog service MySQL database from an EBS volume to an FSx for ONTAP volume using iSCSI presented LUNs.\nTo do that, execute two BackupRestore resources. One recovers all sample-application resources and data except for the catalog-mysql statefulset, and the other recovers and migrates the catalog-mysql statefulset from Amazon EBS to FSx for ONTAP block storage.\nReview the trident-protect-migrate.yaml. Use resourceFilter to exclude and include resources from the recovery process and storageClassMapping to migrate stateful resources to different storage backends.\nresourceFilter: resourceSelectionCriteria: \u0026#34;Exclude\u0026#34; resourceMatchers: - kind: StatefulSet names: [\u0026#34;catalog-mysql\u0026#34;] - kind: TargetGroupBinding IMPORTANT: Make sure you update spec.appArchivePath on both resources. You can retrieve the Archive Path by issuing the following command on your backup:\nkubectl get backup sample-app-backup-1 -n tenant0 -o jsonpath=\u0026#39;{.status.appArchivePath}\u0026#39; You can check the source application and see that the original volume is using Amazon EBS by using the same command:\nkubectl get pvc data-catalog-mysql-0 -n tenant0 Expected result:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE data-catalog-mysql-0 Bound pvc-0d795501-aca2-4e10-98b5-111ecc3aef2c 30Gi RWO ebs-csi \u0026lt;unset\u0026gt; 71m Run the following to create the BackupRestore resources needed for the migration:\ncd \u0026lt;repo\u0026gt;/manifests kubectl create -f trident-protect-migrate.yaml To check if the BackupRestore resources were created successfully run the following command:\nkubectl get backuprestore -n tenant2 IMPORTANT: The state of the BackupRestore can be running while the restore is in progress. Wait until the status gets to Completed. If the status is Failed, use kubectl describe backuprestore -n tenant2 sample-app-migrate-# to get more details on the failure.\nExpected result:\nNAME STATE ERROR AGE sample-app-migrate-1 Completed 26m sample-app-migrate-2 Completed 38s You can check that the catalog-mysql PVC was migrated from Amazon EBS to FSx for ONTAP by issuing the following command:\nkubectl get pvc data-catalog-mysql-0 -n tenant2 Expected result:\nNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS VOLUMEATTRIBUTESCLASS AGE data-catalog-mysql-0 Bound pvc-8e45fa11-13ad-490d-b70c-d649b6fef4e9 30Gi RWO trident-csi-san \u0026lt;unset\u0026gt; 118s After the migration process is finished, you can verify if your migrated application is up and running by accessing the UI service endpoint in your browser. You can get the UI service endpoint by issuing the following command:\nkubectl get svc ui -n tenant2 --output jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; If everything was successful, then you should get the UI that looks like Figure 4:\n![alt text](\nHình 4. Restored retail store application after PVC migration.\nCongratulations! You’ve completed and mastered the basics of Amazon EKS backup, recovery, and migration using NetApp Trident Protect. This guide can be used as a best practice to implement protection, migration, and disaster recovery in your environment.\nCleaning up To avoid unnecessary charges, make sure to delete all of the resources created with Terraform by running the following script from your terminal:\nsh ../scripts/cleanup.sh If you created a new S3 bucket for this exercise, then clean up by going to the Amazon S3 console, emptying the bucket, and deleting it.\nConclusion In conclusion, NetApp Trident Protect provides a powerful solution for streamlining data protection of Kubernetes environments. This approach addresses the critical need for comprehensive backup strategies in cloud-native architectures. NetApp Trident Protect enables users to efficiently create and manage backups of entire namespaces, persistent volumes, and other essential Amazon EKS resources, making sure of business continuity and compliance with data protection requirements.\nThe step-by-step guide presented in this post demonstrates the ease of setting up and using NetApp Trident Protect with Amazon EKS, Amazon FSx for NetApp ONTAP, and Amazon EBS. Thank you for reading this post. Leave any comments or questions in the comments section.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AI-Driven Development Life Cycle: Reimagining Software Engineering Business and technology leaders are constantly striving to improve productivity, increase velocity, foster experimentation, reduce time-to-market (TTM), and enhance the developer experience. These North Star goals drive innovation in software development practices. This innovation is increasingly being powered by artificial intelligence. Particularly, generative AI powered tools such as Amazon Q Developer and Kiro have already begun to revolutionize how software is created. As things stand, organizations employ AI in software development through two primary approaches: AI-assisted development, where AI enhances specific tasks like documentation, code completion, and testing; and AI-autonomous development, where AI is expected to generate entire applications without human intervention based on user requirements. Both these approaches have produced suboptimal results in terms of velocity and software quality that AI-DLC aims to address.\nWhy do we need a transformative approach to AI in software? Our existing software development methods, are designed for human-driven, long running processes, with product owners, developers, architects alike spending most of their time on non-core activities such as planning, meetings, and other software development lifecycle (SDLC) rituals. Simply retrofitting AI as an assistant not only constrains its capabilities but also reinforces outdated inefficiencies. To truly harness AI’s power and achieve the productivity North Star goals, we need to reimagine our entire approach to the software development lifecycle.\nTo achieve transformative results, we need to position AI as a central collaborator and teammate in the development process, and leverage its capabilities throughout the software development lifecycle. This is why we’re introducing the AI-Driven Development Lifecycle (AI-DLC), a new methodology designed to fully ingrain AI capabilities into the very fabric of software development.\nWhat is AI Driven Development Life Cycle (AI-DLC)? AI-DLC is an AI-centric transformative approach to software development that emphasizes two powerful dimensions:\nAI Powered Execution with Human Oversight: AI systematically creates detailed work plans, actively seeks clarification and guidance, and defers critical decisions to humans. This is critical since only humans possess the contextual understanding and knowledge of business requirements needed to make informed choices. Dynamic Team Collaboration: As AI handles the routine tasks, teams unite in collaborative spaces for real-time problem solving, creative thinking and rapid-decision-making. This shift from isolated work to high-energy teamwork accelerates innovation and delivery. These two dimensions enable you to deliver software faster without compromising on quality.\nHow does AI-DLC work? At its core, AI-DLC operates by having AI initiate and direct workflows through a new mental model:\nThis pattern, where AI creates a plan, asks clarifying questions to seek context, and implements solutions only after receiving human validation, repeats rapidly for every SDLC activity, to provide a unified vision and approach for all development pathways.\nWith this mental model at its core, the software development in AI-DLC takes place in three straightforward phases:\nIn the Inception phase, AI transforms business intent into detailed requirements, stories and units through “Mob Elaboration” – where the entire team actively validates AI’s questions and proposals. In the Construction phase, using the validated context from the Inception phase, AI proposes a logical architecture, domain models, code solution and tests through “Mob Construction” – where the team provides clarification on technical decisions and architectural choices in real time. In the Operations phase, AI applies the accumulated context from previous phases to manage infrastructure as code and deployments, with team oversight. Each phase provides richer context for the next, thus enabling AI to provide increasingly informed suggestions.\nAI saves and maintains persistent context across all phases by storing plans, requirements, and design artifacts to your project repository, ensuring seamless continuation of work across multiple sessions.\nAI-DLC introduces new terminology and rituals to reflect its AI-driven, highly collaborative approach. Traditional ‘sprints’ are replaced by ‘bolts’ – shorter, more intense work cycles measured in hours or days rather than weeks; Epics are replaced by Units of Work. This shift in terminology underscores the method’s emphasis on speed and continuous delivery. Similarly, other familiar Agile terms are reimagined to align with the AI-centric workflow, creating a vocabulary that better represents the methodology’s innovative approach to software development.\nWhat are the benefits of this methodology? Velocity: The foremost benefit that AI-DLC offers is acceleration in development velocity, as AI rapidly generates and refines artifacts, such as requirements, stories, designs, code, and tests allowing product owners, architects, and developers to complete tasks in hours or days that previously took weeks. Innovation: Consequently, this acceleration and heavy lifting by AI, frees up significant time for innovation, enabling builders to explore creative solutions and push the boundaries of what’s possible. Quality: With continuous clarification, teams build precisely what they have in mind, rather than an abstract AI interpretation of the intent. This results in products that are more closely aligned with business objectives. AI enhances quality by consistently applying organization-specific standards – your coding practices, design patterns, and security requirements – while generating comprehensive test suites. This end-to-end AI integration improves coherence and traceability from requirements to deployment. Market Responsiveness: The rapid development cycles of AI-DLC enable us to quickly respond to market demands and user feedback, and consequently faster adaptation to requirements. Developer Experience: AI-DLC transforms the developer experience by shifting focus from routine coding tasks to critical problem-solving. AI helps reduce cognitive load by handling repetitive tasks, while satisfaction is enhanced as developers gain deeper business context and witness how their work directly impacts business value. How do I get started with this? Begin your journey with AI-DLC, through three clear paths: Read the comprehensive AI-DLC white paper, explore how Amazon Q Developer rules and Kiro custom workflows can help you implement AI-DLC in your organization consistently or connect with your AWS account team to discuss how AI-DLC can be tailored to your organization’s specific needs.\nThe future of software development is here. We are excited to help you leverage AI to not only build systems faster but also maintain fidelity and quality through critical human oversight and collaboration. Start your AI-DLC journey today and join the growing community of organizations transforming their development practices through AI-driven innovation.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Overcome development disarray with Amazon Q Developer CLI custom agents As a developer who has embraced the power of the Model Context Protocol (MCP)to enhance my workflows, I’m thrilled to see the addition of custom agents in the Amazon Q Developer CLI. This new feature takes the capabilities I’ve come to rely on to a whole new level, allowing me to seamlessly manage different development contexts and easily switch between them.\nIn my previous post, I discussed how MCP servers have revolutionized the way I interact with AWS services, databases, and other essential tools. MCP integration in Amazon Q Developer allows me to query my database schemas, automate infrastructure deployments, and so much more. However, as I started juggling multiple projects, each with their own unique tech stacks and requirements, I found myself needing a more structured approach to managing these diverse development environments.\nEnter custom agents. With this new feature, I can now create and use a custom agent by bringing together specific tools, prompt, context and tool permissions for tasks appropriate for the stage of development. In this post I will explain how to configure a custom agent for front-end and back-end development. Allowing me to easily optimize Amazon Q Developer for each task.\nBackground Imagine that I am working on a multi-tier web application. The application has a React front-end written in Typescript and a FastAPI back-end written in Python. In addition to me, the team includes a designer that uses Figma, and the database administrator that manages a PostgreSQL database. There are subtle differences in how I communicate with the designer and the database administrator. For example, when I discuss a “table” with the designer, I’m likely referring to an HTML table and how the page is structured. However, when I discuss a table with the database administrator, I’m likely talking about a SQL table and how data is stored.\nIn the past, I had both the Figma Dev Mode MCP server and Amazon Aurora PostgreSQL MCP server configured in my environment. While this allowed me to easily work on either the front-end or back-end code, it introduced some challenges. If I asked Amazon Q Developer “how many tables do I have?” Amazon Q Developer would have to guess if I was talking about HTML tables or SQL tables. If the question is about HTML, it should use the Figma server. If the question is about SQL, it should use the Aurora server. This is not a technical limitation, it’s a language limitation. Just as I have to adjust my assumptions to talk with the designer and database administrator, Amazon Q Developer has to make the same adjustments.\nEnter Amazon Q Developer CLI custom agents. Custom agents allow me to optimize Q Developer’s configuration for each scenario. Let’s walk through my front-end and back-end configuration to understand the impact.\nFront-end agent My front-end custom agent is optimized for front-end web development using React and Figma. The following code example is the configuration for my front-end agent stored in ~/.aws/amazonq/cli-agents/front-end.json. Let’s discuss the major sections of the configuration.\nmcpServers – Here I have configured the Figma Dev Mode MCP Server. This simply communicates with the Figma Web Design App installed locally. Note that this replaces the MCP configuration that was stored in ~/.aws/amazonq/mcp.json tools and allowedTools – These two sections are related, so I will discuss them together. tools defines the tools are available to Amazon Q Developer while allowedTools defines which tools are trusted. In other words, Q Developer is able to use all configured tools, and it does not have to ask my permission to use fs_read, fs_write, and @Figma. @Figma allows Amazon Q Developer to use all Figma tools without asking for permission. More on this in the next section. resources – Here I have configured the files that should be added to the context. I have included the README.md (stored in the project folder) and my own preferences for React (stored in my profile). You can read more in the context management section of the user guide. hooks – In addition to the resources, I have also included a hook. This hook will run a command and inject it into the context at runtime. In the example, I am adding the current git status. You can read more in the context hooks section of the user guide. { \u0026#34;name\u0026#34;: \u0026#34;front-end\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Optimized for front-end web development using React and Figma\u0026#34;, \u0026#34;mcpServers\u0026#34;: { \u0026#34;Figma\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;npx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;mcp-remote\u0026#34;, \u0026#34;http://127.0.0.1:3845/sse\u0026#34; ] } }, \u0026#34;tools\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;allowedTools\u0026#34;: [ \u0026#34;fs_read\u0026#34;, \u0026#34;fs_write\u0026#34;, \u0026#34;report_issues\u0026#34;, \u0026#34;@Figma\u0026#34; ], \u0026#34;resources\u0026#34;: [ \u0026#34;file://README.md\u0026#34;, \u0026#34;file://~/.aws/amazonq/react-preferences.md\u0026#34; ], \u0026#34;hooks\u0026#34;: { \u0026#34;agentSpawn\u0026#34;: [ { \u0026#34;command\u0026#34;: \u0026#34;git status\u0026#34; } ] } } Back-end agent My back-end custom agent is optimized for back-end development with Python and PostgreSQL. The following code example is the configuration for my back-end agent stored in ~/.aws/amazonq/cli-agents/back-end.json. Rather than describing the sections, as I did earlier, I will focus on the differences between the front-end and back-end.\nmcpServers – Here I have configured the Amazon Aurora PostgreSQL MCP Server. This allows Amazon Q Developer to query my dev database to learn about the schema. Notice that I have configured a read-only connection to ensure that I don’t accidentally update the database. tools and allowedTools – Once again, I have enabled Amazon Q Developer to use all tools. However, notice that I am more restrictive about what tools are trusted. Amazon Q Developer will need to ask permission to use fs_write or @PostgreSQL/run_query. Notice that I can allow the entire MCP server as I did with Figma or specific tools as I did here. resources – Again, I have included the README.md (stored in the project folder) and my own preferences for Python and SQL (both stored in my profile). Note that I can also use glob patterns here. For example, file://.amazonq/rules/**/*.md would include the rules created by the Amazon Q Developer IDE plugins. hooks – Finally, I have also included the hook for the front-end and back-end. However, I could have included project specific options such as npm run for the front-end and pip freeze for the back-end. { \u0026#34;name\u0026#34;: \u0026#34;back-end\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Optimized for back-end development with Python and PostgreSQL\u0026#34;, \u0026#34;mcpServers\u0026#34;: { \u0026#34;PostgreSQL\u0026#34;: { \u0026#34;command\u0026#34;: \u0026#34;uvx\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;awslabs.postgres-mcp-server@latest\u0026#34;, \u0026#34;--resource_arn\u0026#34;, \u0026#34;arn:aws:rds:us-east-1:xxxxxxxxxxxx:cluster:xxxxxx\u0026#34;, \u0026#34;--secret_arn\u0026#34;, \u0026#34;arn:aws:secretsmanager:us-east-1:xxxxxxxxxxxx:secret:rds!cluster-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx-xxxxxx\u0026#34;, \u0026#34;--database\u0026#34;, \u0026#34;dev\u0026#34;, \u0026#34;--region\u0026#34;, \u0026#34;us-east-1\u0026#34;, \u0026#34;--readonly\u0026#34;, \u0026#34;True\u0026#34; ] } }, \u0026#34;tools\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;allowedTools\u0026#34;: [ \u0026#34;fs_read\u0026#34;, \u0026#34;report_issues\u0026#34;, \u0026#34;@PostgreSQL/get_table_schema\u0026#34; ], \u0026#34;resources\u0026#34;: [ \u0026#34;file://README.md\u0026#34;, \u0026#34;file://~/.aws/amazonq/python-preferences.md\u0026#34;, \u0026#34;file://~/.aws/amazonq/sql-preferences.md\u0026#34; ], \u0026#34;hooks\u0026#34;: { \u0026#34;agentSpawn\u0026#34;: [ { \u0026#34;command\u0026#34;: \u0026#34;git status\u0026#34; } ] } } Using custom agents The real power of agents becomes evident when I need to switch between these different development contexts. I can now simply run q chat --agent front-end when I am working on React and Figma or q chat --agent back-end when I am working with Python and SQL. Amazon Q Developer will configure the correct agent with all my preferences.\nIn the following image, you can see the configuration in the Amazon Q Developer CLI. Notice that the front-end agent has an additional tool called Figma while the back-end agent has an additional tool called PostgreSQL. In addition, the front-end agent trusts fs_write and all of the Figma tools while the back-end agent will ask permission to use fs_write and only trusts one of the two PostgreSQL tools.\nSimilarly, let’s look at the context configuration in both the front-end and back-end agents. In the following image, I have included my React preferences for front-end development, and both Python and SQL preferences for back-end development.\nAs you can see, custom agents allow me to optimize the Amazon Q Developer CLI for each task. Of course, front-end and back-end agents are just an example. You might have a developer and testing agents, data science and analytics agents, etc. Custom agents allow you to tailor the configuration to most any task.\nConclusion Amazon Q Developer CLI custom agents represent a significant improvement in managing complex development environments. By allowing developers to seamlessly switch between different contexts, they eliminate the cognitive overhead of manually reconfiguring tools and permissions for different tasks. Ready to streamline your development workflow? Get started with Amazon Q Developer today.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"How Zapier runs isolated tasks on AWS Lambda and upgrades functions at scale by Anton Aleksandrov, Raúl Negrón-Otero, Ankush Kalra, Vítek Urbanec, and Chandresh Patel on 25 JUL 2025 in Advanced (300), Amazon CloudWatch, Amazon Elastic Kubernetes Service, Architecture, AWS Lambda, Customer Solutions, Monitoring and observability, Serverless\nZapier is a leading no-code automation provider whose customers use their solution to automate workflows and move data across over 8,000 applications such as Slack, Salesforce, Asana, and Dropbox. Zapier runs these automations through integrations called Zaps, which are implemented using a serverless architecture running on Amazon Web Services (AWS). Each Zap is powered by an AWS Lambda fuction.\nIn this post, you’ll learn how Zapier has built their serverless architecture focusing on three key aspects: using Lambda functions to build isolated Zaps, operating over a hundred thousand Lambda functions through Zapier’s control plane infrastructure, and enhancing security posture while reducing maintenance efforts by introducing automated function upgrades and cleanup workflows into their platform architecture.\nArchitecting a secure and isolated runtime environment Zaps created by Zapier’s users implement tenant-specific business logic, hence they require cross-tenant compute isolation. Code implementing one Zap can’t share an execution environment with code implementing another Zap. Moreover, the same Zap type used by two different tenants can’t share execution environments as well.\nTo achieve the required level of isolation, Zapier’s engineering team adopted AWS Lambda, a serverless compute service that runs code in response to events and automatically manages cloud compute resources. Minimal operational overhead, built-in high availability, automated scaling, high level of isolation, and pay-per-use model made Lambda a great fit for this use case. Currently, Zapier’s architecture is running over a hundred thousand Lambda functions to support their customer’s integration workflows Because they’re powered by the open source Firecracker microVMs, each function is completely isolated from the others. Moreover, each execution environment belonging to the same function (sometimes referred to as function instances) is also isolated from other execution environments. The following architecture topology diagram uses red lines to represent isolation boundaries. Each execution environment of every function is isolated from its peers and is getting its own virtual resources such as disk, memory, and CPU. For more details, read Security in AWS Lambda.\n*Image 1. *\nZapier’s control plane is architected using Amazon Elastic Kubernetes Service (Amazon EKS). A designated database is used to maintain the up-to-date function inventory. Whenever a user creates a new Zap, the control plane creates a corresponding Lambda function and stores a reference in the inventory database. When a Zap is triggered, the control plane retrieves information about a relevant Lambda function and invokes it to facilitate the integration workflow, as illustrated in the following diagram.\n*Image 2. *\nUnderstanding the runtime deprecation process When building architectures using the traditional non-serverless compute, cloud engineers are the ones responsible for keeping operating systems and software on their compute instances up to date and applying security and maintenance patches. With serverless architectures and Lambda functions, security patches and minor runtime upgrades are handled by AWS automatically, which means customers can focus on delivering business value instead of the undifferentiated heavy lifting of infrastructure management. When a major Lambda managed runtime version reaches end-of-life, AWS initiates a deprecation process through the AWS Health Dashboard and direct email communications to affected customers. Because deprecated runtimes eventually lose access to security updates and support, organizations must upgrade to supported runtime versions to avoid potential security risks. Read more about the shared responsibility model, runtime use after deprecation, and receiving runtime deprecation notifications.\nAs Zapier’s user base and architectural complexity – and consequently the number of Zaps – were growing, keeping all functions on the most up-to-date major runtime versions became a laborious task. Top contributing factors were:\nHigh number of functions. At its peak, the Zapier platform was running Zaps using hundreds of thousands of unique Lambda functions. Approximately 35% of these functions were using a runtime that was scheduled for deprecation in the next 12 months.\nZapier architected their data plane environment to be ephemeral – the control plane creates and deletes Lambda functions on demand and manages their lifecycle dynamically. Identifying a specific owner for each affected function wasn’t always straightforward.\nSecurity is paramount at Zapier and upgrading affected functions runtime prior to the deprecation date was an absolute must. At no point could Zapier functions use runtimes after their deprecation date. This was a task which required extra resources.\nThe upgrade process shouldn’t have had any impact on the end customer experience. At no point should customer experience be affected. With a short runway, high-volume workload, and the strict requirements of not impacting customer experience, Zapier’s Platform Engineering team took on this challenge of maintaining high security posture in their platform architecture.\nApplying the solution The solution had three work streams:\nReducing the risk by analyzing the architecture and identifying and cleaning up unused functions. Prioritizing upgrades by identifying the most critical and impactful functions. Empowering engineering teams with automated tools and knowledge to streamline the upgrade process in future. Identify and clean up unused functions The first step in streamlining the upgrade process was identifying and removing unused functions. This reduced the total number of functions in Zapier’s architecture that required upgrades, eliminating unnecessary work for the team. Zapier started by augmenting the function inventory with runtime information using AWS Trusted Advisor and Amazon Cloud Intelligence Trusted Advisor dashboards, as illustrated in the following diagram.\n*Image 3. *\nThis meant the team could build a detailed inventory of functions that were running on soon-to-be deprecated runtimes. Using Amazon CloudWatch, Zapier’s platform team started to monitor metrics such as number of invocations. They identified which functions were active, which functions weren’t used for an extended period, and which functions didn’t have an active owner and could be removed. One of the primary mechanisms for ownership validation within the organization was using resource tags. Functions that were active, but didn’t have clear ownership, were flagged for additional review before removal. Functions that were confirmed as unused or didn’t have an active owner were marked for deletion. Removing such functions allowed Zapier to significantly simplify their architecture and reduce the number of functions that had to be upgraded.\nPrioritizing upgrades With a smaller volume of functions to upgrade, Zapier’s platform team prioritized function upgrades based on usage patterns, criticality, and potential customer impact. Three primary prioritization categories were:\nCustomer-facing functions – Any functions directly involved in executing user Zaps were marked as high priority. These had to be upgraded first to avoid service disruptions.\nBackend infrastructure functions – Internal functions that supported system operations were evaluated based on their importance to platform stability.\nHigh-volume functions – Functions with the highest execution frequency were prioritized because upgrading them would have the greatest impact on reducing operational risk.\nUsing these factors, Zapier’s platform team has created an upgrade roadmap, ensuring that critical assets were addressed first while minimizing potential disruptions.\nRefer to Retrieve data about Lambda functions that use a deprecated runtime in the Lambda Developer Guide to learn how to identify most commonly and most frequently used Lambda functions in your serverless architecture.\nEmpowering engineering teams with automated tools and knowledge To ensure a smooth and efficient upgrade process across their serverless architecture, Zapier’s team empowered engineering teams with clear guidelines and automated solutions. The platform incorporated two main approaches: Terraform-managed functions and a custom-built Lambda runtime canary tool. Implementing and adopting these tools and practices resulted in reducing the number of functions using soon-to-be deprecated runtimes by 95%. For functions managed through infrastructure-as-code (IaC), Zapier’s team developed standardized Terraform modules that specified supported runtime versions. Development teams implemented these modules in their configurations:\nresource \u0026#34;aws_lambda_function\u0026#34; \u0026#34;example\u0026#34; { runtime = \u0026#34;python3.13\u0026#34; # Updated to supported runtime } After applying the new module version, teams validated changes by testing the new runtime in staging environments and monitoring Terraform plan outputs to ensure proper runtime version updates. To efficiently manage most Lambda functions in their architecture, Zapier developed the Lambda runtime canary tool suite. Using this solution, they automated the runtime upgrade process for thousands of active Lambda functions with minimal manual intervention. The tool suite implements several key features:\nArchitected for gradual traffic shifting with the Lambda built-in routing mechanism through function version and aliasing. The tool can gradually shift traffic distribution from an old to a new function version. During this gradual traffic shift, the system monitors CloudWatch metrics for errors and automatically rolls back if error rates exceed acceptable thresholds.\nOptimistic upgrade strategy implements direct upgrades for infrequently used functions using a flag value stored in a cache to detect potential issues during the first post-upgrade invocation. If this invocation fails, the control plane retries it using the previous function version. If the retried invocation succeeds, Zapier’s control plane initiates a rollback, assuming the error is most likely due to the runtime upgrade. After rollback, it will log the error and alert relevant stakeholders.\nIntegration with existing infrastructure uses an administrative interface and task queue for automated traffic shifting. A database ledger maintains tracking of function states and rollback information.\nOperational controls provide manual rollback capabilities and implement centralized control switches for process management. After a function was upgraded to a new runtime and no rollback activity was detected within a set time period, an automated pruning task cleans up older versions.\nZapier’s Lambda canary tool, through its integration of gradual traffic shifting, real-time CloudWatch monitoring, and automated rollback mechanisms, established a sustainable framework for managing runtime upgrades across their serverless architecture. This approach not only automated the upgrade process and minimized operational risks but also created a scalable solution that provides continuous runtime upgrades, preventing the use of deprecated runtimes at any point. By allowing continuous function runtime updates with minimal disruption to end user experience, Zapier maintains security and stability while requiring minimal manual intervention. This framework efficiently manages their growing serverless infrastructure, providing both security and operational efficiency for future runtime updates.\nConclusion In this post, you’ve learned how Zapier architected their software-as-a-service (SaaS) platform to provide secure, isolated execution environments using AWS Lambda and Amazon EKS, enabling their customers to create hundreds of thousands of Zaps. You’ve learned how Zapier’s team implemented the function runtime upgrade process at scale and reduced the number of functions running on soon-to-be deprecated runtimes by 95%. You’ve seen best practices that were established and techniques that helped Zapier to keep high security posture without impacting customer experience. Use the following links to learn more about Lambda runtimes and upgrading your functions to the latest runtime versions:\nLambda runtimes documentation Retrieve data about Lambda functions that use a deprecated runtime Managing AWS Lambda runtime upgrades in the AWS Compute Blog AWS Lambda runtime management controls in the AWS Compute Blog "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"How HashiCorp made cross-Region switchover seamless with Amazon Application Recovery Controller by Dmitriy Novikov on 25 JUL 2025 in Amazon Application Recovery Controller (ARC), Customer Solutions\nThis blog was co-authored by Brandon Raabe, Sr. Site Reliability Engineer at HashiCorp.\nIn cloud-based systems, minutes of downtime can translate to significant business impact and eroded customer trust. HashiCorp, a leader in multicloud infrastructure automation software, faced this critical challenge as their HashiCorp Cloud Platform (HCP) scaled to serve enterprise customers with stringent availability requirements. When Regional outages threatened service continuity, the complex dance of failing over DNS entries, workloads, and databases across AWS Regions had become an error-prone process requiring intense coordination. This post chronicles how HashiCorp’s Site Reliability Engineering (SRE) team transformed their disaster recovery capabilities by implementing Amazon Application Recovery Controller (ARC), creating a solution that not only dramatically simplified cross-Region failovers but also provided a standardized way to signal Regional context to their distributed services.\nIn this post, we discuss HashiCorp’s journey from manual, stress-inducing failover procedures to a streamlined, confident approach that fundamentally changed how they deliver on their enterprise-grade resilience promises.\nChallenges with disaster recovery in a multicloud infrastructure HashiCorp’s SRE team recognized that as their cloud platform scaled to serve mission-critical enterprise workloads, their disaster recovery approach needed an upgrade. The existing manual processes required precise coordination across multiple systems during already stressful outage scenarios, which could lead to potential complications when speed and accuracy matter most. Regional outages posed particular challenges: if the control planes for critical services became unavailable, the very tools needed to execute recovery might be inaccessible.\nARC emerged as the ideal solution with its unique architecture: a highly available data plane accessible through endpoints in five distinct Regions, so the recovery mechanism remains operational even during significant Regional disruptions. By using the AWS SDK to interface with ARC, HashiCorp gained several critical advantages. They could apply infrastructure as code (IaC) practices to disaster recovery workflows, automate testing of failover procedures, and integrate resilience seamlessly with their existing operational tooling. This solution transformed their disaster recovery from a specialized manual procedure into a codified, repeatable process embedded within their platform operations.\nRequirements and architectural considerations After evaluating multiple disaster recovery approaches, HashiCorp established three core requirements for their solution. First, while maintaining human judgment for initiating failovers, the execution needed to proceed without additional operator interventions after it was triggered. This human-in-the-loop design preserved deliberate decision-making while reducing error-prone manual steps during implementation.\nSecond, the architecture needed exceptional resilience against the very failures it was designed to mitigate. Traditional DNS failover solutions presented a critical vulnerability: dependency on single-Region control planes that might be unavailable during an outage. ARC solved this problem through its distributed architecture, connecting Amazon Route 53 to a resilient control mechanism, enabled by Route 53 health checks, accessible through multiple Regional endpoints. This means the failover system itself remained available even if the primary Region went offline.\nThird, the solution needed to meet or exceed HashiCorp’s existing Recovery Point Objective (RPO) and Recovery Time Objective (RTO) metrics—the maximum acceptable data loss and downtime thresholds. Using ARC, the SRE team planned to not just reach these targets but make substantial improvements, reducing potential customer impact during Regional events and strengthening HashiCorp’s enterprise-grade resilience.\nSolution overview To transform their disaster recovery posture, HashiCorp’s SRE team designed an architecture centered around ARC and complemented by a purpose-built orchestration service. This architecture seamlessly bridges the human decision to initiate failover with the complex technical operations required to shift traffic between Regions with minimal disruption.\nAt the heart of the solution is a custom failover service that serves as the orchestration layer for Regional transitions. This service maintains configuration details for the ARC cluster and provides a single, controlled interface for initiating Regional switchovers. When activated, the service establishes a secure connection to the ARC API endpoints and executes a two-step workflow: first disabling routing controls for the primary Region, then enabling those for the secondary Region. This sequential approach provides a clean traffic transition without split-brain scenarios or dropped connections.\nThe DNS architecture underwent a strategic evolution to support this new capability. HashiCorp reconfigured their critical ingress endpoints as Route 53 failover record pairs, with each pair consisting of a primary and secondary record. Each record is linked to a health check that monitors the state of an ARC routing control—effectively connecting AWS’s global DNS service to the ARC routing control. The primary records resolve to endpoints in the primary Region, and secondary records point to corresponding infrastructure in the standby Region. When routing controls change state, the associated health checks automatically trigger Route 53 to adjust DNS resolution patterns, redirecting traffic to the appropriate Regional infrastructure.\nHashiCorp maintains their secondary Region in a warm standby configuration, with essential services running but not actively serving client traffic until a failover event occurs. To enable seamless awareness of Region status across their distributed system, the team implemented a signaling mechanism using specially crafted TXT DNS records. These records are tied to the same ARC routing controls as the primary service endpoints, effectively creating a discoverable, global state indicator. Services can query these TXT records to dynamically determine the currently active Region and adjust their internal routing, replication, and operational behaviors accordingly — alleviating the need for a separate configuration distribution system and making sure all components have a consistent view of the current Regional state. The following diagram illustrates the disaster recovery workflow.\n*Hình 1. *\nThis architecture combines human oversight for initiating critical Regional transitions with fully automated execution after the decision is made. The use of ARC’s globally distributed control plane removes single-Region dependencies that might otherwise compromise the failover mechanism itself during a Regional outage event.\nOperational decision framework for Regional failover HashiCorp’s Regional failover process balances automated monitoring with deliberate human decision-making. Their comprehensive observability platform continuously monitors Regional health, automatically alerting the incident response team when anomalies are detected. When alerts trigger, the incident management protocol activates, with an incident commander quickly assembling experts to assess the situation.\nThe team follows a structured evaluation framework to determine if failover is warranted: confirming the issue is Region-specific, verifying that redundant intra-Region components can’t mitigate the problem, and assessing whether the projected Regional recovery time exceeds acceptable customer impact thresholds. This approach prevents unnecessary Regional transitions while providing rapid action when genuinely needed.\nAfter the decision to failover is made, an authorized operator initiates the process through a single API call to their orchestration service, which then interfaces with ARC to execute the complex sequence of routing control changes. This design preserves human judgment for the critical decision while using automation for precise execution, so HashiCorp can respond confidently and consistently during high-pressure Regional outage scenarios.\nDisaster recovery testing HashiCorp maintains operational readiness through a disciplined monthly disaster recovery testing program in their integration environment. One week before each scheduled test, the team notifies all stakeholders to confirm organization-wide awareness and participation. On test day, they follow formal incident protocols, creating dedicated communication channels for transparent observation and collaboration.\nThe test execution mirrors their production failover process: an operator initiates the recovery sequence through their API, activating the ARC routing controls to shift traffic to the secondary Region. What sets HashiCorp’s approach apart is their comprehensive validation methodology. The team verifies critical services in the secondary Region and then fails back to the primary Region with subsequent validation. This bidirectional testing confirms both failover and failback procedures work reliably.\nEach exercise concludes with a structured retrospective where the team documents observations and identifies improvement opportunities. By treating these tests as learning experiences rather than compliance activities, HashiCorp has established a continuous improvement cycle for their disaster recovery capabilities. The insights from these regular drills have led to numerous refinements in their ARC implementation and operational procedures, so their team can respond confidently during actual outages with practiced, predictable procedures.\nConclusion The collaboration between HashiCorp and AWS through ARC has revolutionized HashiCorp’s disaster recovery capabilities. Regional transitions that once required careful DNS record manipulation by specialized operators now execute through a single API call, with traffic shifting within seconds and full propagation completing in approximately 2 minutes. This dramatic simplification, achieved by integrating the resilient ARC architecture with HashiCorp’s custom orchestration service, has not only improved recovery metrics but has also strengthened their enterprise-grade resilience promises.\nARC has solved a fundamental distributed systems challenge by providing a reliable mechanism for services to determine the active Region. By linking ARC routing controls to specialized TXT records, HashiCorp created a consistent global indicator that allows services to automatically adjust their behavior without additional coordination systems—simplifying their architecture and reducing dependencies. Most significantly, this implementation has democratized disaster recovery within HashiCorp, transforming it from a specialized capability to a standardized procedure executable by their regular on-call rotation. The solution’s highly available endpoints across multiple Regions makes sure the recovery mechanism itself remains operational even during severe outages—addressing a critical vulnerability in their previous approach.\nFor HashiCorp’s enterprise customers, these improvements translate directly to business value: reduced recovery times during Regional events, increased operational confidence, and assurance that their critical infrastructure management tools will remain available even during major cloud disruptions. As HashiCorp continues to refine their approach through rigorous testing and continuous improvement, their ARC implementation demonstrates how thoughtfully architected disaster recovery can evolve from merely an insurance policy into a strategic competitive advantage.\nTo learn more, visit Amazon Application Recovery Controller, AWS Multi-Region Capabilities and AWS Multi-Region Fundamentals.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.6-blog6/","title":"Blog 5","tags":[],"description":"","content":"Implementing message prioritization with quorum queues on Amazon MQ for RabbitMQ by Akhil Melakunta and Vinodh Kannan Sadayamuthu on 23 JUL 2025 inAdvanced (300), Amazon MQ, Serverless, Technical How-to\nQuorum queues are now available on Amazon MQ for RabbitMQ from version 3.13. Quorum queues are a replicated First-In, First-Out (FIFO) queue type that uses the Raft consensus algorithm to maintain data consistency. Quorum queues on RabbitMQ version 3.13 lack one key feature compared to classic queues: message prioritization. However, RabbitMQ version 4.0 introduced support for message priority, which behaves differently than classic queue message priorities. Migrating applications from classic queues with message priority to quorum queues on Amazon MQ for RabbitMQ presents challenges for customers. This post describes the different approaches to implementing message prioritization in quorum queues in Amazon MQ for RabbitMQ.\nAmazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that simplifies setting up and operating message brokers on AWS.\nWhy message prioritization matters Modern messaging systems require handling messages differently, depending on the business priority. Some messages are more time-sensitive or critical than others and prioritizing them can enhance the efficiency and responsiveness of applications. Message prioritization allows certain messages to be processed before others, aligning with business priorities and helping to ensure that high-value or time-critical messages receive the attention they need.\nMessage prioritization addresses critical business challenges across multiple industries. In insurance companies, it can expedite urgent claim processing by prioritizing high-priority messages over routine policy updates, reducing settlement times. Automotive manufacturers can make sure that critical production line alerts and safety notifications take precedence over standard telemetry data, preventing costly downtime. Energy utilities can prioritize real-time grid stability alerts and outage notifications, enabling faster responses to potential blackouts. By implementing message priority, industries can direct immediate attention to time-sensitive operations while efficiently managing routine processes within existing infrastructure. By using this approach to transform their communication strategies, organizations can respond more quickly and effectively to critical events.\nClassic queues compared to quorum queues message prioritization In this section, explore the fundamental differences between classic queues and quorum queues when it comes to message prioritization capabilities. Examine how each queue type handles message priority, the built-in features available, and key considerations.\nMessage prioritization with classic queues In classic queues, RabbitMQ supports message priorities ranging from 1 to 255, with 1 being the lowest priority and 255 being the highest. However, it’s generally recommended to use a smaller range (for example, 1–5) for better performance, because RabbitMQ needs to maintain an internal sub-queue for each priority from 1 up to the maximum value configured for a given queue. A wider priority range adds more CPU and memory cost, which can impact broker performance. Priority queue behavior in classic queues:\nClassic queues require x-max-priority argument to define the maximum number of priorities for a given queue A procedure sends a message with a priority property value Consumers don’t need special configuration to handle priorities Messages with higher priority are delivered before messages with lower priority Within the same priority level, messages are delivered in FIFO order Messages without a priority property are treated as if their priority is lowest Messages with a priority that is higher than the queue’s maximum are treated as if they were published with the maximum priority *Image 1. *\nExample Python code for classic queue implementation with message priority:\n#!/usr/bin/env python import pika import ssl # Set up SSL context for secure connection context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2) # Define credentials credentials = pika.PlainCredentials(\u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;) # Replace with actual credentials # Set up connection parameters for Amazon MQ RabbitMQ broker connection_parameters = pika.ConnectionParameters( host=\u0026#39;b-example.mq.us-west-2.on.aws\u0026#39;, # Replace with actual broker endpoint port=5671, credentials=credentials, ssl_options=pika.SSLOptions(context) ) # Establish connection and create a channel connection = pika.BlockingConnection(connection_parameters) channel = connection.channel() # Declare a direct exchange # - direct exchanges route messages based on routing key channel.exchange_declare( exchange=\u0026#39;priority_exchange\u0026#39;, exchange_type=\u0026#39;direct\u0026#39;, ) # Declare a priority queue # - x-max-priority=5 sets maximum priority level (0-5) # - x-queue-type=classic specifies classic queue implementation channel.queue_declare( queue=\u0026#39;classic_priority_queue\u0026#39;, arguments={ \u0026#39;x-max-priority\u0026#39;: 5, \u0026#39;x-queue-type\u0026#39;: \u0026#34;classic\u0026#34; } ) # Bind queue to exchange with routing key # - This connects the queue to the exchange # - Messages sent to the exchange with matching routing key will be routed to this queue channel.queue_bind( queue=\u0026#39;classic_priority_queue\u0026#39;, exchange=\u0026#39;priority_exchange\u0026#39;, routing_key=\u0026#39;priority_queue\u0026#39; ) # Publish messages with different priorities # Low priority message (priority=1) channel.basic_publish( exchange=\u0026#39;priority_exchange\u0026#39;, routing_key=\u0026#39;priority_queue\u0026#39;, body=\u0026#39;Low priority message\u0026#39;, properties=pika.BasicProperties(priority=1) ) print(\u0026#34; [x] Sent \u0026#39;Low priority message\u0026#39;\u0026#34;) # Medium priority message (priority=2) channel.basic_publish( exchange=\u0026#39;priority_exchange\u0026#39;, routing_key=\u0026#39;priority_queue\u0026#39;, body=\u0026#39;Medium priority message\u0026#39;, properties=pika.BasicProperties(priority=2) ) print(\u0026#34; [x] Sent \u0026#39;Medium priority message\u0026#39;\u0026#34;) # High priority message (priority=5) channel.basic_publish( exchange=\u0026#39;priority_exchange\u0026#39;, routing_key=\u0026#39;priority_queue\u0026#39;, body=\u0026#39;High priority message\u0026#39;, properties=pika.BasicProperties(priority=5) ) print(\u0026#34; [x] Sent \u0026#39;High priority message\u0026#39;\u0026#34;) # Close the connection connection.close() The preceding code demonstrates message prioritization in RabbitMQ using a classic queue with built-in priority handling. The implementation connects to a RabbitMQ broker using the Python Pika library and declares a direct exchange, a classic queue with a maximum priority level of 5. Messages are then published to this single queue with explicitly assigned priority values (1 for low, 2 for medium, and 5 for high priority). When consumers fetch messages from this queue, RabbitMQ will deliver higher priority messages first.\nMessage prioritization with quorum queues Unlike classic queues, quorum queues in Rabbit MQ 3.13 don’t support message prioritization natively. However, there are effective patterns that you can implement to achieve message priority with Quorum queues.\nUsing separate queues for different priorities A straightforward method is to create multiple quorum queues, each dedicated to different priority levels. For example, you might have a high-priority queue and a low-priority queue. Using RabbitMQ exchange and binding key route messages to the appropriate queues based on their priority, allowing the system to process high-priority messages more promptly, as shown in the following figure.\n*Image 2. *\nExample to implement priority handling using separate quorum queues:\n#!/usr/bin/env python import pika import ssl # Set up SSL context for secure connection context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2) # Define credentials credentials = pika.PlainCredentials(\u0026#39;username\u0026#39;, \u0026#39;password\u0026#39;) #Replace with actual credentials # Set up connection parameters for Amazon MQ RabbitMQ broker connection_parameters = pika.ConnectionParameters( host=\u0026#39;b-example.mq.us-west-2.on.aws\u0026#39;, port=5671, credentials=credentials, ssl_options=pika.SSLOptions(context) ) # Establish connection and create a channel connection = pika.BlockingConnection(connection_parameters) channel = connection.channel() # Declare a direct exchange # - Direct exchanges route messages based on routing key channel.exchange_declare( exchange=\u0026#39;priority_exchange_qq\u0026#39;, exchange_type=\u0026#39;direct\u0026#39; ) # Create separate quorum queues for different priority levels # Low priority queue channel.queue_declare( queue=\u0026#39;low_priority_queue\u0026#39;, durable=True, arguments={ \u0026#39;x-queue-type\u0026#39;: \u0026#34;quorum\u0026#34; } ) # Bind the low priority queue to the exchange with a specific routing key # - This creates a rule that messages sent to \u0026#39;priority_exchange\u0026#39; with routing_key=\u0026#39;low_priority_1\u0026#39; # - will be routed to the \u0026#39;low_priority_queue\u0026#39; channel.queue_bind( queue=\u0026#39;low_priority_queue\u0026#39;, exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;low_priority_1\u0026#39; ) # Medium priority queue channel.queue_declare( queue=\u0026#39;medium_priority_queue\u0026#39;, durable=True, arguments={ \u0026#39;x-queue-type\u0026#39;: \u0026#34;quorum\u0026#34; } ) # Bind the medium priority queue to the exchange with a specific routing key # - Messages with routing_key=\u0026#39;medium_priority_2\u0026#39; will be directed to the \u0026#39;medium_priority_queue\u0026#39; channel.queue_bind( queue=\u0026#39;medium_priority_queue\u0026#39;, exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;medium_priority_2\u0026#39; ) # High priority queue channel.queue_declare( queue=\u0026#39;high_priority_queue\u0026#39;, durable=True, arguments={ \u0026#39;x-queue-type\u0026#39;: \u0026#34;quorum\u0026#34; } ) # Bind the high priority queue to the exchange with a specific routing key # - Messages with routing_key=\u0026#39;high_priority_2\u0026#39; will be directed to the \u0026#39;high_priority_queue\u0026#39; channel.queue_bind( queue=\u0026#39;high_priority_queue\u0026#39;, exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;high_priority_5\u0026#39; ) # Publish messages to different priority queues print(\u0026#34; [x] Publishing messages to different priority queues\u0026#34;) # Low priority message channel.basic_publish( exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;low_priority_1\u0026#39;, body=\u0026#39;Low priority message\u0026#39; ) print(\u0026#34; [x] Sent \u0026#39;Low priority message\u0026#39;\u0026#34;) # Medium priority message channel.basic_publish( exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;medium_priority_2\u0026#39;, body=\u0026#39;Medium priority message\u0026#39; ) print(\u0026#34; [x] Sent \u0026#39;Medium priority message\u0026#39;\u0026#34;) # High priority message channel.basic_publish( exchange=\u0026#39;priority_exchange_qq\u0026#39;, routing_key=\u0026#39;high_priority_5\u0026#39;, body=\u0026#39;High priority message\u0026#39; ) print(\u0026#34; [x] Sent \u0026#39;High priority message\u0026#39;\u0026#34;) # Close the connection connection.close() print(\u0026#34; [x] Connection closed\u0026#34;) The preceding code demonstrates a message prioritization approach in RabbitMQ using separate quorum queues for different priority levels (low, medium, and high). The implementation uses the Python Pika library to connect to a RabbitMQ server, a direct exchange and three separate quorum queues for different priority levels, and publish messages to different routing keys with different priority.\nCustom priority logic on consumers Implement custom logic within your application to handle messages based on their priority. For example, you can use headers or metadata to determine the priority of a message and then use this information to route messages to different queues or handle them in a specific order. Higher priority queues should use more consumers or consumers with higher resources allocated to process messages more quickly than lower priority queues. Use the basic.qos (prefetch) method in manual acknowledgement mode on your consumers to limit the number of messages that can be out for delivery at any time and allow messages to be prioritized. basic.qos is a value a consumer sets when connecting to a queue. It indicates how many messages the consumer can handle at one time. This method is shown in the following figure.\n*Image 3. *\nNote: This solution implements message priority on a best-effort basis. There is a possibility that low and medium priority messages may be processed before high priority messages.\nConclusion Message prioritization in RabbitMQ brokers on Amazon MQ has different considerations for classic and quorum queues. Using quorum queues requires a thoughtful approach because of the lack of native support for message proritization in RabbitMQ. By employing separate queues and custom logic, you can achieve effective prioritization while maintaining the high availability and consistency that quorum queues offer. Embrace these strategies to optimize your messaging infrastructure, enhance application responsiveness, and make sure that critical messages are processed in a timely manner. We recommend that you adopt quorum queues as the preferred replicated queue type on RabbitMQ 3.13 brokers. For more details, see Amazon MQ documentation. For more information, see quorum queues. To learn more, see Amazon MQ for Rabbit MQ.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.7-blog7/","title":"Blog 7","tags":[],"description":"","content":"Streamlining AWS Serverless workflows: From AWS Lambda orchestration to AWS Step Functions This blog post discusses the AWS Lambda as orchestrator anti-pattern and how to redesign serverless solutions using AWS Step Functions with native integrations.\nStep Functions is a serverless workflow service that you can use to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines. Step Functions provides native integrations with over 200 AWS services in addition to external third-party APIs. You can use these integrations to deploy production-ready solutions with less effort, reducing code complexity, improving long-term maintainability, and minimizing technical debt when operating at scale.\nThe Lambda as orchestrator anti-pattern Let’s examine a common anti-pattern: using a Lambda function as an orchestrator for message distribution across multiple channels. Consider this real-world scenario where a system needs to send notifications through SMS or email channels based on user preferences, as shown in the following diagram.\nThe payload examples for this scenario are:\nSend SMS only: { \u0026#34;body\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;sms\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hello from AWS Lambda!\u0026#34;, \u0026#34;phoneNumber\u0026#34;: \u0026#34;+1234567890\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;priority\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;notification\u0026#34; } } } Send email only: { \u0026#34;body\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hello from AWS Lambda!\u0026#34;, \u0026#34;email\u0026#34;: { \u0026#34;to\u0026#34;: \u0026#34;recipient@example.com\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;Test Notification\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;sender@example.com\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;priority\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;notification\u0026#34; } } } Send both SMS and email: { \u0026#34;body\u0026#34;: { \u0026#34;channel\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hello from AWS Lambda!\u0026#34;, \u0026#34;email\u0026#34;: { \u0026#34;to\u0026#34;: \u0026#34;recipient@example.com\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;Test Notification\u0026#34;, \u0026#34;from\u0026#34;: \u0026#34;sender@example.com\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;priority\u0026#34;: \u0026#34;normal\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;notification\u0026#34; } } } Here’s how it typically starts—with a Lambda function acting as an orchestrator:\nimport boto3 import json # Initialize Lambda client # You can specify region if needed: boto3.client(\u0026#39;lambda\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39;) lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) def lambda_handler(event, context): try: # Parse the incoming event body = json.loads(event[\u0026#39;body\u0026#39;]) # Validate required fields if \u0026#39;channel\u0026#39; not in body: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Missing channel parameter\u0026#39;) } if \u0026#39;message\u0026#39; not in body: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Missing message content\u0026#39;) } if body[\u0026#39;channel\u0026#39;] == \u0026#39;both\u0026#39;: # Invoke SMS Lambda function lambda_client.invoke( FunctionName=\u0026#39;send-sns\u0026#39;, InvocationType=\u0026#39;Event\u0026#39;, Payload=json.dumps(body) ) # Invoke Email Lambda function lambda_client.invoke( FunctionName=\u0026#39;send-email\u0026#39;, InvocationType=\u0026#39;Event\u0026#39;, Payload=json.dumps(body) ) else: # Validate channel value if body[\u0026#39;channel\u0026#39;] not in [\u0026#39;sms\u0026#39;, \u0026#39;email\u0026#39;]: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Invalid channel specified\u0026#39;) } # Invoke function based on specified channel function_name = \u0026#39;send-sns\u0026#39; if body[\u0026#39;channel\u0026#39;] == \u0026#39;sms\u0026#39; else \u0026#39;send-email\u0026#39; lambda_client.invoke( FunctionName=function_name, InvocationType=\u0026#39;Event\u0026#39;, Payload=json.dumps(body) ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Messages sent successfully\u0026#39;) } except json.JSONDecodeError: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps(\u0026#39;Invalid JSON in request body\u0026#39;) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: json.dumps(f\u0026#39;Error: {str(e)}\u0026#39;) } This approach has the following problems:\nIntrinsicComplex error handling: The orchestrator needs to manage errors from multiple function invocations. Tight coupling: Functions are directly dependent on each other. Limited execution time: The orchestrator Lambda function continues running while sub Lambda functions execute. This could lead to the orchestrator Lambda function timing out. Idle resources:Because the orchestrator Lambda function is sitting idle waiting for returns from other Lambda functions, in this case, the user is now paying for idle resources. Rearchitecting with Step Functions You can rebuild the logic using Step Functions and Amazon States Language to replace the Lambda orchestrator function. You can use the Choice state in Amazon States Language to define logical conditions to follow a specific path. This approach reduces code maintenance complexity because you define the conditions using Amazon States Language. You can also use it to to extend the functionality with minimal changes to the codebase.\nThe following Step Functions workflow diagram shows the rearchitected version of the previous Orchestrator Lambda function:\nThe following Amazon State Language represents the workflow:\n{ \u0026#34;Comment\u0026#34;: \u0026#34;Multi-channel notification workflow\u0026#34;, \u0026#34;StartAt\u0026#34;: \u0026#34;ValidateInput\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;ValidateInput\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;And\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.message\u0026#34;, \u0026#34;IsPresent\u0026#34;: true }, { \u0026#34;Variable\u0026#34;: \u0026#34;$.channel\u0026#34;, \u0026#34;IsPresent\u0026#34;: true } ], \u0026#34;Next\u0026#34;: \u0026#34;DetermineChannel\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;ValidationError\u0026#34; }, \u0026#34;ValidationError\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Fail\u0026#34;, \u0026#34;Error\u0026#34;: \u0026#34;ValidationError\u0026#34;, \u0026#34;Cause\u0026#34;: \u0026#34;Required fields missing: message and/or channel\u0026#34; }, \u0026#34;DetermineChannel\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Choice\u0026#34;, \u0026#34;Choices\u0026#34;: [ { \u0026#34;Variable\u0026#34;: \u0026#34;$.channel\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;both\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;ParallelNotification\u0026#34; }, { \u0026#34;Variable\u0026#34;: \u0026#34;$.channel\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;sms\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SendSMSOnly\u0026#34; }, { \u0026#34;Variable\u0026#34;: \u0026#34;$.channel\u0026#34;, \u0026#34;StringEquals\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;SendEmailOnly\u0026#34; } ], \u0026#34;Default\u0026#34;: \u0026#34;FailState\u0026#34; }, \u0026#34;ParallelNotification\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Parallel\u0026#34;, \u0026#34;Branches\u0026#34;: [ { \u0026#34;StartAt\u0026#34;: \u0026#34;SendSMS\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;SendSMS\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::sns:publish\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Message.$\u0026#34;: \u0026#34;$.message\u0026#34;, \u0026#34;PhoneNumber.$\u0026#34;: \u0026#34;$.phoneNumber\u0026#34; }, \u0026#34;End\u0026#34;: true } } }, { \u0026#34;StartAt\u0026#34;: \u0026#34;SendEmail\u0026#34;, \u0026#34;States\u0026#34;: { \u0026#34;SendEmail\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FromEmailAddress.$\u0026#34;: \u0026#34;$.email.from\u0026#34;, \u0026#34;Destination\u0026#34;: { \u0026#34;ToAddresses.$\u0026#34;: \u0026#34;States.Array($.email.to)\u0026#34;, \u0026#34;CcAddresses.$\u0026#34;: \u0026#34;States.ArrayGetItem(States.JsonToString($.email.cc), $)\u0026#34;, \u0026#34;BccAddresses.$\u0026#34;: \u0026#34;States.ArrayGetItem(States.JsonToString($.email.bcc), $)\u0026#34; }, \u0026#34;Content\u0026#34;: { \u0026#34;Simple\u0026#34;: { \u0026#34;Subject\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.email.subject\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; }, \u0026#34;Body\u0026#34;: { \u0026#34;Text\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.message\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; }, \u0026#34;Html\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.email.htmlBody\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } }, \u0026#34;ReplyToAddresses.$\u0026#34;: \u0026#34;States.Array($.email.replyTo)\u0026#34;, \u0026#34;EmailTags\u0026#34;: [ { \u0026#34;Name\u0026#34;: \u0026#34;channel\u0026#34;, \u0026#34;Value\u0026#34;: \u0026#34;email\u0026#34; }, { \u0026#34;Name\u0026#34;: \u0026#34;messageType\u0026#34;, \u0026#34;Value.$\u0026#34;: \u0026#34;$.email.messageType\u0026#34; } ], \u0026#34;ConfigurationSetName.$\u0026#34;: \u0026#34;$.email.configurationSet\u0026#34;, \u0026#34;ListManagementOptions\u0026#34;: { \u0026#34;ContactListName.$\u0026#34;: \u0026#34;$.email.contactList\u0026#34;, \u0026#34;TopicName.$\u0026#34;: \u0026#34;$.email.topic\u0026#34; } }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:sesv2:sendEmail\u0026#34;, \u0026#34;End\u0026#34;: true } } } ], \u0026#34;End\u0026#34;: true }, \u0026#34;SendSMSOnly\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::sns:publish\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;Message.$\u0026#34;: \u0026#34;$.message\u0026#34;, \u0026#34;PhoneNumber.$\u0026#34;: \u0026#34;$.phoneNumber\u0026#34; }, \u0026#34;End\u0026#34;: true }, \u0026#34;SendEmailOnly\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FromEmailAddress.$\u0026#34;: \u0026#34;$.email.from\u0026#34;, \u0026#34;Destination\u0026#34;: { \u0026#34;ToAddresses.$\u0026#34;: \u0026#34;States.Array($.email.to)\u0026#34; }, \u0026#34;Content\u0026#34;: { \u0026#34;Simple\u0026#34;: { \u0026#34;Subject\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.email.subject\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; }, \u0026#34;Body\u0026#34;: { \u0026#34;Text\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.message\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; }, \u0026#34;Html\u0026#34;: { \u0026#34;Data.$\u0026#34;: \u0026#34;$.email.htmlBody\u0026#34;, \u0026#34;Charset\u0026#34;: \u0026#34;UTF-8\u0026#34; } } } } }, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::aws-sdk:sesv2:sendEmail\u0026#34;, \u0026#34;End\u0026#34;: true }, \u0026#34;FailState\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Fail\u0026#34;, \u0026#34;Cause\u0026#34;: \u0026#34;Invalid channel specified\u0026#34; } } } This Step Functions implementation offers several advantages:\nNative service integration: Direct integration with Amazon Simple Notification Service (Amazon SNS), Amazon Simple Email Service (Amazon SES), Amazon DynamoDB, and Amazon CloudWatch eliminates the need for wrapper Lambda functions Visual workflow: The execution flow is visible and maintainable through the AWS Management Console Built-in error handling: Retry policies and error states can be defined declaratively Parallel execution: The Parallel state handles multiple channel delivery efficiently Simplified logic: The Choice state replaces complex if-else statements Centralized data flow: Input and output are managed consistently across states Enhanced workflow duration capabilities: Step Functions Standard workflows support executions that run for up to one year, compared to the 15-minute maximum execution time for Lambda functions Comparing Lambda function as orchestrator to Step Functions The summary of different features implemented on Lambda function as orchestrator and Step Functions is reflected in the following table:\nFeature Lambda function as orchestrator Step Functions Orchestration logic Implemented in Python with nested if-else statements. Defined declaratively using the Choice state Multi-channel delivery Sequential function invocations. Parallel execution using function\u0026rsquo;s logic. Parallel execution using the Parallel state Service integration Requires SDK calls or separate Lambda functions. Direct integration with AWS services (Amazon SNS, DynamoDB) Error handling Custom try-except blocks in Python. Built-in error states and retry policies Data persistence Custom code to interact with DynamoDB. Native DynamoDB integration with putItem task Metrics logging Custom code to call CloudWatch. CloudWatch Metrics SDK integration Implementation considerations Review the following considerations when re-architecting a Lambda function orchestrator to Step Functions:\nState machine type: Choose between Standard (up to 1 year runtime) and Express (up to 5 minutes) workflows based on your needs. Input/output management: Parameters manipulation reduces the development effort and give flexible alternatives to implement the workflow: Parameters: Selects specific input fields to pass to the next state ResultSelector: Filters the state response to include only relevant fields ResultPath: Stores the processed result in a specific path of the state input OutputPath: Determines what data passes to the next state A code snippet for these features is:\n{ \u0026#34;ProcessOrder\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;Task\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:states:::lambda:invoke\u0026#34;, \u0026#34;Parameters\u0026#34;: { \u0026#34;FunctionName\u0026#34;: \u0026#34;ProcessOrderFunction\u0026#34;, \u0026#34;Payload\u0026#34;: { \u0026#34;orderId.$\u0026#34;: \u0026#34;$.orderId\u0026#34;, \u0026#34;customerId.$\u0026#34;: \u0026#34;$.customerId\u0026#34; } }, \u0026#34;ResultSelector\u0026#34;: { \u0026#34;orderStatus.$\u0026#34;: \u0026#34;$.Payload.status\u0026#34;, \u0026#34;processedDate.$\u0026#34;: \u0026#34;$.Payload.timestamp\u0026#34; }, \u0026#34;ResultPath\u0026#34;: \u0026#34;$.orderProcessing\u0026#34;, \u0026#34;OutputPath\u0026#34;: \u0026#34;$.orderProcessing\u0026#34;, \u0026#34;Next\u0026#34;: \u0026#34;NotifyCustomer\u0026#34; } } Error handling: Implement retry policies and catch errors at both the task and state machine levels. Monitoring: Set up CloudWatch logs and metrics for your state machine to track executions and performance. Benefits of using Step Functions Using Step Functions for rearchitecting scenarios bring the following benefits:\nReduced code complexity: The business logic is now defined in Amazon States Language rather than distributed across multiple Lambda functions. Improved maintainability: Developers can make workflow changes by modifying the Amazon States Language, often modifying several Lambda functions. Native AWS service integrations: Step Functions offers direct integrations with over 200 AWS services, which you can use to connect and coordinate AWS resources without writing custom integration code. Cost optimization: By using direct service integrations, there are fewer Lambda invocations and reduced costs. Long-running processes: Step Functions can manage workflows that run for up to a year, beyond the 15-minute limit for Lambda functions. Conclusion Rearchitecting Lambda-based applications with Step Functions can significantly improve maintainability, scalability, and operational efficiency. By moving orchestration logic into Step Functions and using its native service integrations, you can create more robust and manageable serverless applications.\nWhile this post focused on a message distribution workflow, the principles apply to many serverless architectures. As you develop your applications, consider how Step Functions can help you build more resilient and scalable solutions.\nTo learn more about serverless architectures visit Serverless Land.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.8-blog8/","title":"Blog 8","tags":[],"description":"","content":"Maximizing Business Value Through Strategic Cloud Optimization As cloud adoption continues to accelerate, organizations are realizing that the journey to the cloud is just the beginning. The real challenge—and opportunity—lies in optimizing cloud usage to drive maximum business value. At AWS, we’re committed to helping our customers navigate this journey successfully. Let’s explore some key insights and best practices for cloud optimization from the recent MIT Technology Review publication, Driving business value by optimizing the cloud.\nThe cloud optimization imperative Recent data shows that global cloud infrastructure spending reached $84 billion in Q3 2024, marking a 23% year-over-year increase. This growth underscores the critical role of the cloud in driving business agility and innovation. However, to truly harness the power of the cloud, organizations must strike the right balance between cost, security, resilience, and innovation.\nAndré Dufour, AWS Director and General Manager for AWS Cloud Optimization, emphasizes that cloud optimization involves making cloud spending efficient so that freed-up resources can be redirected to fund new innovations, such as generative AI initiatives.\nCloud optimization should be viewed as a continuous process rather than a one-time event, requiring regular assessment whenever business conditions or technical requirements change significantly. The approach should be comprehensive, addressing not just costs but all six pillars (operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability), while also recognizing that different workloads require tailored optimization strategies rather than a one-size-fits-all approach.\nBest practices for success Consider the following best practices:\nUpskill your team – Empower your employees with cloud, cost management, and optimization skills. As Dufour notes, “Every engineer or builder plays a role in cloud optimization.” Establish a cloud center of excellence – Create a centralized body responsible for developing and distributing cloud best practices throughout your organization. Align finance and business – Make cloud KPIs business-centric rather than purely technical, so cloud optimization efforts support overall business goals. Embrace automation – Use tools to automate cloud provisioning, monitoring, and optimization, reducing human error and effort. Use AI services and solutions for efficiency – Use AI technologies to automate visualization, enhance decision-making, and optimize resource utilization. Real-world success stories Our customers are already seeing significant benefits from strategic cloud optimization:\nDreamCasino achieved 30% cost savings and a 50% reduction in API response times, enabling expansion into new markets BMC Software reduced cloud costs by 25% while improving security and reliability, reinvesting savings into new business opportunities Even within AWS, our use of Amazon Q for application modernization saved an estimated 4,500 years of development work and $260 million in performance benefits Business impact Effective cloud optimization delivers more than just cost savings. It enables the following:\nFaster innovation through reinvestment of saved resources Enhanced security and operational efficiency Improved ability to scale and adapt to business needs Better customer experiences and faster time-to-market The capability to make informed architecture and design decisions by balancing trade-offs across AWS Well-Architected pillars AWS resources for your optimization journey To help you accelerate your cloud optimization efforts, AWS provides several tools and resources:\nAWS Cloud Adoption Framework – Assess your cloud readiness AWS Well-Architected Framework – Detailed guidance across all six pillars AWS Well-Architected Tool – Evaluate your workloads against AWS best practices AWS Well-Architected Lenses – Extend AWS Well-Architected Framework to specific industry and technology domains, including the Generative AI lens Well-Architected IaC (Infrastructure as Code) Analyzer tool – Automatically assess IaC templates such as AWS CloudFormation and Terraform against the AWS Well-Architected Framework Getting started To get started, consider the following steps:\nDownload the MIT executive report Assess your cloud readiness using the Cloud Adoption Framework Begin with a Well-Architected Review to assess your current state Use AWS Trusted Advisor to optimize costs, improve performance, and address security gaps Subscribe to AWS Health events to learn how service and resource changes might affect your applications running on AWS Consider AWS Managed Services to extend your team with operational capabilities such as monitoring, incident management, AWS Incident Detection and Response, security, patch, backup, and cost optimization Use AWS re:Post as an authoritative, knowledge-sharing service designed to help you quickly remove technical roadblocks, accelerate innovation, and operate more efficiently Additionally, you can engage with the AWS Cloud Optimization Success (COS) team for more detailed guidance and to help identify what to do next in your cloud optimization journey. The COS team has Solutions Architects who specialize in the Cloud Adoption Framework and Well-Architected Framework and deliver workshops and training sessions though customer and partner engagements. The team can help drive adoption of AWS services through the use of the Well-Architected and Cloud Adoption Frameworks and support other services like AWS Trusted Advisor and AWS Health to optimize cost and cloud architectures. Whether you’re just starting or looking to enhance existing implementations, the AWS COS team provides the guidance, tools, and expertise you need to succeed.\nConclusion At AWS, we’re dedicated to helping you optimize your cloud journey. By implementing these strategies and best practices, you can unlock the full potential of the cloud, driving innovation and growth while maintaining security and operational excellence.\nReady to take your cloud optimization to the next level? Refer to the resources included in this post and contact your AWS COS team to learn how we can help you maximize the value of your cloud investments.\nAbout the Authors Ryan Dsouza Ryan Dsouza is a Principal Solutions Architect in the Cloud Optimization Success organization at AWS. Based in New York City, Ryan helps customers design, develop, and operate more secure, scalable, and innovative solutions using the breadth and depth of AWS capabilities to deliver measurable business outcomes. He is actively engaged in developing strategies, guidance, and tools to support customers architect solutions that optimize for performance, cost-efficiency, security, resilience, and operational excellence, adhering to the AWS Cloud Adoption Framework and Well-Architected Framework.\nAnitha Selvan Anitha Selvan is a Go-To-Market Lead for the Cloud Optimization Success organization. With over 8 years of experience in product marketing and orchestrating go-to-market strategies across AWS support products, she specializes in product launches, go-to-market motions, messaging, and positioning that support both business growth and product adoption.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/3.9-blog9/","title":"Blog 9","tags":[],"description":"","content":"Introducing v2 of Powertools for AWS Lambda (Java) Modern applications increasingly rely on Serverless technologies such as Amazon Web Services (AWS) Lambda to provide scalability, cost efficiency, and agility. The Serverless Applications Lens for the AWS Well-Architected Framework focuses on how to design, deploy, and architect your Serverless applications to overcome some of these challenges.\nPowertools for AWS Lambda is a developer toolkit that helps you implement Serverless best practices and directly translates AWS Well-Architected recommendations into actionable, developer friendly utilities. Following the community’s continued successful adoption of Powertools for AWS in Python, Java, TypeScript, and .NET, this post announces the general availability of Powertools for AWS Lambda (Java) v2 coming with major performance improvements, enhanced core utilities, and a brand-new Kafka utility.\nPowertools for AWS (Java) v2 provides three updated core utilities:\nLogging: A re-designed Java idiomatic logging module providing structured logging that streamlines log aggregation and analysis. Metrics: An improved metrics experience allowing custom metrics collection using CloudWatch Embedded Metric Format (EMF). Tracing: An annotation-based way to collect distributed tracing data with AWS X-Ray to visualize and analyze request flows. Along with the updated core utilities, v2 of the developer toolkit adds two brand new features:\nGraalVM native image support: Native image support for GraalVM across all core utilities reducing Lambda cold start times up to 75.61% (p95). Kafka utility: This new utility integrates with Amazon Managed Streaming for Apache Kafka (Amazon MSK) and self-managed Kafka event sources on Lambda and allows developers to deserialize directly into Kafka native types such as ConsumerRecords. Learn more about how to migrate to v2 in our upgrade guide.\nGetting started using Powertools for AWS Lambda (Java) v2 Powertools for AWS Lambda (Java) v2 is readily accessible as a Java package on Maven Central and integrates with popular build tools such as Maven and Gradle. This post focuses on Maven-based implementation samples to help you get started quickly. Gradle examples are available for all utilities in the documentation and the examples repository.\nThe toolkit is compatible with Java 11 and newer versions, making sure you can use modern Java features while building Serverless applications. Examples on how to install each utility are outlined in each section of the post and complete configuration examples are also available in the Powertools documentation.\nLogging The Logging utility helps implement structured logging when running on Lambda while still using familiar Java logging libraries such as slf4j, log4j, and logback. v2 of Logging allows you to do the following:\nOutput structured JSON logs enriched with Lambda context Choose the logging backend of your choice among log4j2 and logback Add structured arguments to logs that get serialized into arbitrarily nested JSON objects Add global log keys using the slf4j default Mapped Diagnostic Context (MDC) To add the logging utility to your project, include it as a dependency in your Java Maven project. The following example shows how to add the log4j2 logging backend to your application:\n\u0026lt;!-- In the dependencies section --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-logging-log4j\u0026lt;/artifactId\u0026gt; \u0026lt;!-- Alternatively, if you wish to use the logback backend \u0026lt;artifactId\u0026gt;powertools-logging-logback\u0026lt;/artifactId\u0026gt; --\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- In the build plugins section --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;dev.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectj-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;aspectLibraries\u0026gt; \u0026lt;aspectLibrary\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-logging\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/aspectLibrary\u0026gt; \u0026lt;/aspectLibraries\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; Create a custom JsonTemplateLayout appender in your log4j2.xml file:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;Configuration\u0026gt; \u0026lt;Appenders\u0026gt; \u0026lt;Console name=\u0026#34;JsonAppender\u0026#34; target=\u0026#34;SYSTEM_OUT\u0026#34;\u0026gt; \u0026lt;JsonTemplateLayout eventTemplateUri=\u0026#34;classpath:LambdaJsonLayout.json\u0026#34; /\u0026gt; \u0026lt;/Console\u0026gt; \u0026lt;/Appenders\u0026gt; \u0026lt;Loggers\u0026gt; \u0026lt;Logger name=\u0026#34;JsonLogger\u0026#34; level=\u0026#34;INFO\u0026#34; additivity=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;JsonAppender\u0026#34;/\u0026gt; \u0026lt;/Logger\u0026gt; \u0026lt;Root level=\u0026#34;info\u0026#34;\u0026gt; \u0026lt;AppenderRef ref=\u0026#34;JsonAppender\u0026#34;/\u0026gt; \u0026lt;/Root\u0026gt; \u0026lt;/Loggers\u0026gt; \u0026lt;/Configuration\u0026gt; To add structured logging to your functions, apply the @Logging annotation to your Lambda handler and use the familiar slf4j Java API when writing log statements. This allows you to adopt the logging utility without major code refactoring. Powertools handles routing to the correct logging backend for you. The following example shows how to add global log keys using MDC, and add a structured entry argument to your log message:\npublic class App implements RequestHandler\u0026lt;SQSEvent, String\u0026gt; { private static final Logger log = LoggerFactory.getLogger(App.class); @Logging public String handleRequest(final SQSEvent input, final Context context) { // Add a global log key using Mapped Diagnostic Context MDC MDC.put(\u0026#34;myCustomKey\u0026#34;, \u0026#34;willBeLoggedForAllLogStatements\u0026#34;); // Log a message with a structured argument (any JSON serializable Object) log.info(\u0026#34;My message\u0026#34;, entry(\u0026#34;anotherCustomKey\u0026#34;, Map.of(\u0026#34;nested\u0026#34;, \u0026#34;object\u0026#34;))); // ... return response } } Lambda sends the following JSON-formatted output to Amazon CloudWatch Logs (note how the Java Map gets auto-serialized into a JSON object):\n{ \u0026#34;level\u0026#34;: \u0026#34;INFO\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;My message\u0026#34;, \u0026#34;cold_start\u0026#34;: true, \u0026#34;function_arn\u0026#34;: \u0026#34;arn:aws:lambda:us-east-1:012345678912:function:AppFunction\u0026#34;, \u0026#34;function_memory_size\u0026#34;: 512, \u0026#34;function_name\u0026#34;: \u0026#34;AppFunction\u0026#34;, \u0026#34;function_request_id\u0026#34;: \u0026#34;0150a2a4-c5aa-4277-9345-17bad039f6c0\u0026#34;, \u0026#34;function_version\u0026#34;: \u0026#34;$LATEST\u0026#34;, \u0026#34;sampling_rate\u0026#34;: 0.1, \u0026#34;service\u0026#34;: \u0026#34;powertools-java-sample\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2025-05-20T08:35:28.565Z\u0026#34;, \u0026#34;myCustomKey\u0026#34;: \u0026#34;willBeLoggedForAllLogStatements\u0026#34;, \u0026#34;anotherCustomKey\u0026#34;: { \u0026#34;nested\u0026#34;: \u0026#34;object\u0026#34; } } Metrics CloudWatch offers essential built-in service metrics for monitoring application throughput, error rates, and resource usage. Users also need to capture workload specific custom metrics relevant to their business use-case following AWS Well-Architected best-practices.\nPowertools for AWS (Java) enables you to create custom metrics asynchronously by outputting metrics in CloudWatch EMF directly to standard output—an approach that needs no other configuration. The Lambda service sends the EMF formatted metrics to CloudWatch on your behalf.\nThe Metrics utility allows you to:\nCreate custom metrics asynchronously using CloudWatch EMF Reduce latency by avoiding synchronous metric publishing Automatically track cold starts in a custom CloudWatch metric Avoid manually validating your output against the EMF specification Keep you code clean by avoiding manual flushing to standard output To add the Metrics utility to your project, add the following Maven dependency:\n\u0026lt;!-- In the dependencies section --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-metrics\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- In the build plugins section --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;dev.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectj-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;aspectLibraries\u0026gt; \u0026lt;aspectLibrary\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-metrics\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/aspectLibrary\u0026gt; \u0026lt;/aspectLibraries\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; To add custom metrics to your Lambda function, place the @FlushMetrics annotation on your Lambda handler. The library takes care of validating and flushing your metrics to standard output before the Lambda function terminates. The following example shows how you can automatically capture a cold start metric and emit your own custom metrics:\npublic class App implements RequestHandler\u0026lt;SQSEvent, String\u0026gt; { private static final Logger log = LoggerFactory.getLogger(App.class); private static final Metrics metrics = MetricsFactory.getMetricsInstance(); // This configures a default namespace and service dimension for all metrics @FlushMetrics(namespace = \u0026#34;ServerlessAirline\u0026#34;, service = \u0026#34;payment\u0026#34;, captureColdStart = true) public String handleRequest(final SQSEvent input, final Context context) { // The Metrics instance is a singleton metrics.addMetric(\u0026#34;CustomMetric1\u0026#34;, 1, MetricUnit.COUNT); // Publish metrics with non-default configuration options DimensionSet dimensionSet = new DimensionSet(); dimensionSet.addDimension(\u0026#34;Service\u0026#34;, \u0026#34;AnotherService\u0026#34;); metrics.flushSingleMetric(\u0026#34;CustomMetric2\u0026#34;, 1, MetricUnit.COUNT, \u0026#34;AnotherNamespace\u0026#34;, dimensionSet); // ... return response } } Figure 1. AWS CloudWatch Metrics Graph View.\nTracing The Tracing utility provides an annotation-based integration with X-Ray for distributed tracing with minimal configuration. Tracing allows you to:\nGain visibility into your own methods calls and AWS service interactions visualized in the X-Ray console Automatically capture method responses and errors Automatically capture Lambda cold start information as part of your traces Add custom metadata to traces for more context and debugging information Enable or disable tracing features through environment variables without code changes To add the Tracing utility to your project, add the following Maven dependency:\n\u0026lt;!-- In the dependencies section --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-tracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- In the build plugins section --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;dev.aspectj\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;aspectj-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;aspectLibraries\u0026gt; \u0026lt;aspectLibrary\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-tracing\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/aspectLibrary\u0026gt; \u0026lt;/aspectLibraries\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; To enable tracing in your Lambda function, annotate your Lambda handler and your custom methods that you want to trace with the @Tracing annotation. Each annotation maps to a sub-segment of your main Lambda handler in X-Ray and becomes visible in the console.\npublic class App implements RequestHandler\u0026lt;APIGatewayProxyRequestEvent, APIGatewayProxyResponseEvent\u0026gt; { private static final Logger log = LoggerFactory.getLogger(App.class); @Tracing public APIGatewayProxyResponseEvent handleRequest(final APIGatewayProxyRequestEvent input, final Context context) { // ... business logic // Get calling IP with tracing String location = getCallingIp(\u0026#34;https://checkip.amazonaws.com\u0026#34;); // ... return response } @Tracing(segmentName = \u0026#34;Location service\u0026#34;) private String getCallingIp(String address) { // Implementation to get IP address log.info(\u0026#34;Retrieving caller IP address\u0026#34;); // Add custom metadata to current sub-segment URL url = new URL(address); putMetadata(\u0026#34;getCallingIp\u0026#34;, address); // ... return \u0026#34;127.0.0.1\u0026#34;; } } The X-Ray console displays a generated service map when traffic begins flowing through your application. Applying the Tracing annotation to your Lambda function handler method or any other methods in the execution chain provides you with comprehensive visibility into the traffic patterns throughout your application. The following figure shows how the custom metadata added in the example is associated with the custom sub-segment.\nFigure 2. AWS X-Ray waterfall trace view\nReducing Lambda cold start duration A key feature in Powertools for AWS Lambda (Java) v2 is GraalVM native image support for all core utilities. Compiling your Lambda functions to native executables allows you to significantly reduce cold start times and memory usage. Using Powertools v2 with GraalVM allows you to reduce cold starts up to 75.61% (p95) compared to using the managed Java runtime. The following benchmark compares the cold start times of an application using all core utilities (logging, metrics, tracing) on the managed java21 runtime as compared to the Lambda provided.al2023 runtime running a GraalVM compiled native image (go to the supported Lambda runtimes):\nEnvironment p95 (ms) Min (ms) Avg (ms) Max (ms) Max Memory (MB) N Powertools for AWS (Java) v2: JVM 1682.92 1224.55 1224.55 2229.81 205.04 234 Powertools for AWS (Java) v2: GraalVM 542.86 404.92 504.77 752.85 93.46 369 This improvement is particularly valuable for latency-sensitive applications and functions that scale frequently. Check out a full working example on GitHub.\nLambda MSK Event Source Mapping Integration The new Kafka utility introduced with Powertools for AWS Lambda (Java) v2 streamlines working with the Lambda MSK Event Source Mapping (ESM) and self-managed Kafka event sources. It provides a familiar experience for developers working with Apache Kafka by allowing direct conversion from Lambda events to Kafka’s native types. The key features include:\nDirect deserialization into Kafka ConsumerRecords\u0026lt;K, V\u0026gt; objects while using the Lambda-native RequestHandler interface Support for deserializing JSON, Avro, and Protobuf encoded records for key and value fields with and without usage of a Schema Registry when producing the messages To add the Kafka utility to your project, include the powertools-kafka library as a Maven dependency in your pom.xml:\n\u0026lt;!-- In the dependencies section --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;software.amazon.lambda\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;powertools-kafka\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- Kafka clients dependency - compatibility works for \u0026gt;= 3.0.0 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-clients\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.0.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Use the @Deserialization annotation on your Lambda handler to deserialize messages as native Kafka ConsumerRecords. Make sure to specify the deserializer type. The following example shows how to deserialize Avro encoded record values with String keys. As in a regular Lambda handler, declare the input type to your function in the RequestHandler generic parameters and the utility discovers the deserialization types automatically. The AvroProduct class in the following example is an auto-generated Java class using the Java org.apache.avro.avro library.\npublic class App implements RequestHandler\u0026lt;ConsumerRecords\u0026lt;String, AvroProduct\u0026gt;, Void\u0026gt; { private static final Logger log = LoggerFactory.getLogger(App.class); @Deserialization(type = DeserializationType.KAFKA_AVRO) public Void handleRequest(ConsumerRecords\u0026lt;String, AvroProduct\u0026gt; consumerRecords, Context context) { log.info(\u0026#34;Deserialized {} records.\u0026#34;, consumerRecords.records().size()); // ... Business logic return null; } } Conclusion Powertools for AWS Lambda (Java) v2 represents the next evolution in the toolkit for building robust, observable, and high-performing Serverless applications. Throughout this post, we’ve explored the enhanced core observability utilities with their new features, the performance gains through GraalVM native image support, and the new Kafka utility that supports using familiar Kafka patterns when working on Lambda.\nPowertools also offers more utilities to handle common Serverless design patterns. Each utility is designed with the same principles of clarity and minimal overhead.To learn more:\nVisit the documentation for detailed guides and examples Try the sample applications Join the community on GitHub to share your experience and get help Your next Serverless application awaits with Powertools for AWS Lambda (Java) v2. We would love to hear your feedback!\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyễn Hoàng Mai Vy\nPhone Number: 0862498257\nEmail: vynguyen08257@gmail.com\nUniversity: Saigon University\nMajor: Information Technology\nClass: AWS092025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 29/9/2025 to 22/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members in the First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be deployed this week: Day Task Start Date Completion Date Resource Mon - Get acquainted with FCJ members - Read and note the rules and regulations at the internship unit 29/09/2025 29/09/2025 Tue - Research AWS and its service categories + Compute + Storage + Networking + Database + \u0026hellip; 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ Wed - Create an AWS Free Tier account - Explore AWS Console \u0026amp; AWS CLI - Practice: + Create an AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ Thu - Research basic EC2: + Instance types + AMI + EBS + Security Group + \u0026hellip; - Methods to remotely SSH into EC2 - Research Elastic IP 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ Fri - Practice: + Create an EC2 instance (T2.micro, Amazon Linux 2, new Key Pair) + Configure Security Group to open SSH port + SSH connection from personal machine + Attach EBS volume \u0026amp; mount to EC2 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Outcomes: Relationships: Got acquainted and connected with the members of the First Cloud Journey (FCJ) group. Basic AWS Knowledge: * Understood what AWS is and grasped the basic service categories: * Compute (EC2), Storage (S3, EBS), Networking (VPC, Security Group), Database (RDS), etc. AWS Account \u0026amp; Tools Practice: * Successfully created and configured an AWS Free Tier account. * Became familiar with the AWS Management Console and learned how to find, access, and use services from the web interface. * Installed and configured AWS CLI on the computer including: * Default Access Key, Secret Key, Region. EC2 Usage: * Grasped the basic concepts of EC2 (Instance types, AMI, EBS, Security Group). * Successfully created an EC2 instance (t2.micro) and established an SSH connection from a personal machine. * Learned how to manage and attach an EBS volume to the EC2 instance. AWS CLI Usage: * Used AWS CLI to perform basic operations such as: * Checking account and configuration information (e.g., aws configure list). * Getting a list of regions (e.g., aws ec2 describe-regions). * Viewing EC2 services (e.g., aws ec2 describe-instances). * Creating and managing key pairs. Summary: Capable of connecting between the web interface and CLI to manage AWS resources in parallel. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: In-depth research on AWS VPC. Understand network security models in AWS. Grasp advanced networking services: VPN, Direct Connect, Load Balancer. Tasks to be deployed this week: Day Task Start Date Completion Date Resource Mon - Introduction and overview of AWS VPC - Research VPC components: Subnet, Route Table, Internet Gateway, NAT Gateway 06/10/2025 06/10/2025 https://000003.awsstudygroup.com/ Tue - Research VPC Security: + Security Groups + Network ACLs + Layer 3/4 security models 07/10/2025 07/10/2025 https://000003.awsstudygroup.com/ Wed - Research VPC connectivity mechanisms: + Site-to-Site VPN + Client VPN 08/10/2025 08/10/2025 https://000003.awsstudygroup.com/ Thu - Research AWS Direct Connect: + Architecture + Benefits + When to use Direct Connect 09/10/2025 09/10/2025 https://000003.awsstudygroup.com/ Fri - Research Elastic Load Balancer (ELB): + Classic LB + Application LB + Network LB - Load balancing architecture in a system 10/10/2025 10/10/2025 https://000003.awsstudygroup.com/ Week 2 Outcomes: Grasped the AWS VPC architecture, including: * Subnets (Public / Private) * Route Tables * Internet Gateway / NAT Gateway * DHCP, VPC CIDR\nUnderstood the security components within VPC: * Security Groups (stateful) * Network ACLs (stateless) * Layered security model in a system.\nUnderstood VPC connectivity methods: * Site-to-Site VPN * Client VPN * Pros and cons of each type.\nUnderstood AWS Direct Connect: * Dedicated physical connection from on-premises to AWS * Used for systems requiring high bandwidth and strong security.\nGrasped Load Balancer knowledge: * Application Load Balancer (Layer 7) * Network Load Balancer (Layer 4) * High Availability \u0026amp; Fault Tolerance architecture.\nCapable of analyzing AWS network models and proposing appropriate architectures for different use cases.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Understand AWS storage services. Know how to deploy a Backup system, perform virtual machine Import/Export, and implement Storage Gateway. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about Amazon Simple Storage Service (S3) với tính năng Access Point và Storage Class của S3 - Learn about S3 Static Website \u0026amp; CORS, Control Access, Object Key \u0026amp; Performance, Glacier 20/10/2025 20/10/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html 3 - Learn about Snow Family, Storage Gateway, Backup 21/10/2025 21/10/2025 https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html 4 - Practice: Deploy AWS Backup to the System + Create S3 Bucket và Deploy Infrastructure + Create Backup Plan, set up notifications and test restore 22/10/2025 22/10/2025 https://000013.awsstudygroup.com/ 5 - Practice: VM Import/Export\n+ VMWare WorkStation + Import virtual machine to AWS + Export EC2 Instance from AWS 23/10/2025 23/10/2025 https://000014.awsstudygroup.com/ 6 - Practice: + Using File Storage Gateway: Create Storage Gateway, File Shares and Mount File shares on On-premises machine + Amazon FSx for Windows File Server 24/10/2025 24/10/2025 https://000024.awsstudygroup.com/ Week 4 Achievements: Understand what Amazon Simple Storage Service (S3) is and grasp its core feature groups: Amazon S3 Access Point S3 Static Website \u0026amp; CORS Access Control, Object Key \u0026amp; Performance, Glacier Understand the Snow Family, Storage Gateway, and Backup services. Created and configured an S3 Bucket, deployed the related infrastructure, set up notifications, and verified successful operation. Successfully created a Storage Gateway and File Shares, with the ability to connect file shares from On-premise machines. Able to import virtual machines into AWS and export EC2 Instances from AWS: Export virtual machines from On-premise Upload virtual machines to AWS Launch EC2 Instances from AMI Able to export EC2 Instances from AWS: Configure ACL for the S3 Bucket Export virtual machines from EC2 Instances Set up a shared data storage system for the Windows infrastructure: Create a practice environment to set up a new file share Monitor and evaluate performance Enable components required for deploying FSx on Windows such as user storage quotas, continuous access sharing, etc. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand core AWS security services (IAM, SSO, Cognito, KMS, Organizations). Practice configuring Security Hub, optimizing EC2, and managing resources using Tags/IAM. Set up IAM Users/Groups/Roles, Permission Boundaries, and define project objectives and system architecture. Select the project topic and identify the project requirements. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Familiarize with AWS security services: + Shared Responsibility Model + Amazon Cognito + AWS Organization + AWS Identity Center (SSO) + AWS KMS 03/11/2025 03/11/2025 3 - Practice: + Enable Security Hub and perform assessments based on each security standard + Optimizing EC2 Costs with AWS Lambda 04/11/2025 04/11/2025 https://000018.awsstudygroup.com/ https://000022.awsstudygroup.com/ 4 - Practice: + Manage Resources using Tags and Resource Groups + Manage access to EC2 services using resource tags with AWS IAM 05/11/2025 05/11/2025 https://000027.awsstudygroup.com/ https://000028.awsstudygroup.com/ 5 - - Practice: + Limitation of user rights with IAM permission boundary + Encrypt at rest with AWS KMS 06/11/2025 06/11/2025 https://000030.awsstudygroup.com/ https://000033.awsstudygroup.com/ 6 - Select the project topic and define project objectives - Design the system architecture - Practice: + Create IAM Groups and IAM Users, and configure Role Conditions + Granting authorization for an application to access AWS services with an IAM role 07/11/2025 09/11/2025 https://000044.awsstudygroup.com/ https://000048.awsstudygroup.com/ Week 5 Achievements: Understand and grasp core AWS security service groups: Shared Responsibility Model Amazon Identity and Access Management Amazon Cognito AWS Organizations AWS Identity Center (SSO) AWS Key Management Service (KMS) Practice and operate security services Enable and become familiar with AWS Security Hub. Perform security checks and assessments according to various standards and AWS Best Practices. Identify security findings and determine their handling priority. Optimize EC2 costs using AWS Lambda Use Lambda to automatically start/stop EC2 instances on a schedule. Identify unnecessary resources that can be optimized. Manage AWS resources using Tags and Resource Groups Establish a standardized tagging system (Environment, Owner, Project, CostCenter). Apply tags to EC2, S3, Lambda, and other resources. Create Resource Groups to monitor and manage resources by group or project. Manage access using IAM and Tag Conditions Configure IAM policies with conditions based on resource tags. Limit EC2 access through Resource Tags to separate environments and reduce operational risks. Restrict user permissions with Permission Boundaries Configure Permission Boundaries for IAM Users/Roles. Ensure users cannot assign themselves permissions beyond the allowed limit. Encrypt data using AWS KMS Configure at-rest encryption for S3, EBS, and related services. Create and manage keys, and control key usage through KMS policies. Project topic selection and scope definition Select the project topic and define project objectives, scope, and expected deliverables. System architecture design Draft the overall architecture of the application/project. Identify AWS services used, and define authorization, encryption, and logging workflows. Advanced IAM practice Create IAM Groups and IAM Users for each functional team. Configure IAM Roles and define access conditions (IP/Tag/Time). Grant application access to AWS services using IAM Roles. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Set up the project architecture using a Monorepo structure. Build the initial infrastructure using IaC (Infrastructure as Code) with AWS SAM. Complete the Backend MVP for the CREATE room function. Design UI wireframes/mockups for the Frontend direction. Set up basic CI/CD for upcoming deployments. Tasks for this Week: Day Tasks Start Date Completion Date References 2 - Create Monorepo structure: backend/, frontend/, infrastructure/ - Complete template.yaml (IaC): DynamoDB + 2 Lambdas (CRUD \u0026amp; Search) - Run sam build 03/11/2025 03/11/2025 https://docs.aws.amazon.com/serverless-application-model/ 3 - Backend MVP: Implement CREATE function in roomCrud.js - Configure package.json (aws-sdk, node-fetch) 04/11/2025 04/11/2025 https://docs.aws.amazon.com/lambda/ 4 - Design UI wireframe/mockups for Home, Search, and Post Room pages (no coding UI) 05/11/2025 05/11/2025 https://www.figma.com 5 - Set up CI/CD: + Configure AWS Secrets on GitHub + Write initial deploy-backend.yml 06/11/2025 06/11/2025 https://docs.github.com/en/actions Week 6 Achievements: Successfully created a well-structured Monorepo with backend/, frontend/, and infrastructure/. Completed the IaC file template.yaml: 1 DynamoDB Table: Rooms 2 Lambda Functions: roomCrud.js and searchRooms.js Successfully ran sam build, confirming the infrastructure is ready for deployment. Completed Backend MVP for CREATE Room function. Designed wireframes/mockups for Home, Search, and Post Room pages. Completed initial CI/CD setup: Added AWS Secrets to GitHub Created draft deploy-backend.yml Overall Result: The IaC foundation and CREATE API were successfully defined and prepared for deployment. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Deploy AWS infrastructure (Lambda, API Gateway, DynamoDB). Build the Frontend MVP (Base UI) and the Post Room form. Integrate APIs across Frontend → API Gateway → Lambda → DynamoDB. Implement basic backend functions for room search. Tasks for this Week: Day Tasks Start Date Completion Date References 2 - Deploy AWS infrastructure using sam deploy - Create Lambda, API Gateway, DynamoDB - Add API Gateway Endpoint to frontend/.env.local 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Build UI for Layout (layout.js) - Build UI for Post Room Page (post-room/page.js) - Install \u0026amp; configure Tailwind CSS 11/11/2025 11/11/2025 4 - Implement API Proxy in Next.js at frontend/api/proxy/ 12/11/2025 12/11/2025 https://nextjs.org/docs 5 - Integrate Post Room form with Lambda roomCrud.js via API Proxy 13/11/2025 13/11/2025 https://docs.aws.amazon.com/lambda/ 6 - Implement backend search function: create searchRooms.js (READ – get all rooms) 14/11/2025 14/11/2025 https://docs.aws.amazon.com/amazondynamodb/ 7 - Test entire flow: Frontend → API Proxy → API Gateway → Lambda → DynamoDB - Fix bugs \u0026amp; finalize MVP 15/11/2025 15/11/2025 Week 7 Achievements: Successfully deployed AWS infrastructure via sam deploy. Configured Frontend environment (.env.local) with API Gateway endpoint. Completed base UI: Layout Post Room page Tailwind CSS working smoothly Implemented API Proxy in Next.js to forward requests to Lambda. Fully integrated Post Room form: Submit → API Proxy → API Gateway → Lambda → DynamoDB. Completed backend search function (searchRooms.js – READ). System successfully: Loads website Allows posting rooms Stores data in DynamoDB "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Integrate map display and geocoding for room locations. Complete search and filtering functionality. Test all major features of the system. Prepare Proposal Review if required. Tasks for this Week: Day Tasks Start Date Completion Date References 2 - Backend: Integrate Vietmap Geocoding API into roomCrud.js to store coordinates - Frontend: Add Vietmap-gl to RoomMap.js 17/11/2025 17/11/2025 https://maps.vietmap.vn/ 3 - Backend: Complete filtering logic (price, district, distance) in searchRooms.js 18/11/2025 18/11/2025 https://docs.aws.amazon.com/amazondynamodb/ 4 - Frontend: Build Search Page (search/page.js) with filters - Display results on map + list 19/11/2025 19/11/2025 https://nextjs.org/docs 5 - Manual Testing: posting rooms (with coordinates), searching, filtering 20/11/2025 20/11/2025 6 - Prepare Proposal Review: summarize completed features \u0026amp; presentation materials 21/11/2025 21/11/2025 Week 8 Achievements: Successfully integrated Vietmap Geocoding: Auto-fetch coordinates when users post a room Store coordinates in DynamoDB via Lambda Frontend map display using Vietmap-gl works smoothly. Completed search and filtering: Price filter District filter Distance-based filter Improved search logic in Lambda Search Page completed: UI filters Room list Map with markers Testing results: Posting → geocoding → storage works correctly Searching \u0026amp; filtering functions correctly UI + API synced across modules Final Result: Search + Map modules are fully functional — the core of the app is now complete. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learning AWS VPC, network security, and advanced networking services.\nWeek 3: Mastering EC2, instance types, management tools, EBS, and Auto Scaling for operating virtual machine resources on AWS.\nWeek 4: Mastering AWS storage services, including Backup deployment, VM Import/Export, and using Storage Gateway.\nWeek 5: Gaining solid knowledge of AWS security services (Security Hub, IAM), optimizing resources with Tags, and selecting/configuring the project architecture.\nWeek 6: Setting up Monorepo architecture, building infrastructure with AWS SAM (IaC), completing the Backend MVP for the CREATE Room function, designing mockups, and setting up basic CI/CD.\nWeek 7: Deploying AWS infrastructure (Lambda, API Gateway, DynamoDB), building the Frontend MVP (Base UI/Form), integrating APIs, and implementing the room search function.\nWeek 8: Integrating map/geocoding, completing search and filtering features, and conducting full testing of core functionalities.\nWeek 9: Performing task I\u0026hellip;\nWeek 10: Performing task L\u0026hellip;\nWeek 11: Performing task M\u0026hellip;\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Objectives of Week 10: Set up a mechanism to export data from DynamoDB to S3 for analysis. Continue fixing bugs and optimizing the system. Tasks Implemented During the Week: Day Tasks Start Date End Date References 2 Research data export mechanisms from DynamoDB to S3 01/12/2025 01/12/2025 https://docs.aws.amazon.com/amazondynamodb/ 3 Develop script to export DynamoDB data to S3 (JSON/CSV) 02/12/2025 02/12/2025 https://docs.aws.amazon.com/AmazonS3/ 4 Configure scheduling for periodic data export 03/12/2025 03/12/2025 https://docs.aws.amazon.com/scheduler/ 5 Fix Backend issues: APIs, data handling, authorization 04/12/2025 04/12/2025 6 Fix Frontend issues: UI rendering, forms, user experience 05/12/2025 05/12/2025 https://nextjs.org/docs 7 System-wide testing after bug fixing and data export implementation 06/12/2025 06/12/2025 Results of Week 10: Successfully implemented data export from DynamoDB to S3: Data stored in JSON/CSV format. Used for periodic backups and data analytics. Completed Backend and Frontend bug fixing: Stabilized core APIs. Improved user interface behavior. Conducted full system testing after updates. Overall Outcome: The system is more stable, and the data backup \u0026amp; analytics foundation is fully established. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Objectives of Week 9: Implement user authentication using AWS Cognito. Develop chat and favorite features for user interaction. Optimize UI for responsive design. Tasks Implemented During the Week: Category Detailed Tasks Deliverables Authentication (Auth) * AWS Cognito Setup: Create User Pool and App Client. * Frontend: Integrate AWS Amplify into Next.js. * Build UI for Sign Up / Sign In / Forgot Password. * User Roles: Implement basic authorization (User vs Owner) using Cognito Groups. Users can successfully register and log in, and the system correctly distinguishes user roles. Chat \u0026amp; Favorite * Backend: Complete logic in chatMessage.js (save/load messages between two users) and favorite.js (add/remove favorite rooms). * Frontend: Build Chat Modal/Page (ChatModal.js, chat/page.js) for message sending/receiving. * Integrate Favorite button on RoomCard.js. Chat and Favorite features work reliably and smoothly. UI Optimization * Ensure all main pages are fully responsive (Mobile-first design). User-friendly interface on all devices. Results of Week 9: Authentication system using AWS Cognito is fully operational. Successfully implemented real-time chat interaction between users. Users can save favorite rooms. UI optimized for mobile, tablet, and desktop experiences. Overall Outcome: User interaction features are complete, enhancing overall system usability. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Roommate Unified AWS Serverless Solution for Real-Time Room Management and Search 1. Operational Summary RoomMate is a room search and management platform designed for students and landlords, leveraging Serverless and AWS Free Tier architecture. The platform allows landlords to easily post listings, manage room data (add, edit, delete), and receive recurring reports. Students can search, filter by multiple criteria (location, price, amenities), save favorite rooms, and chat directly with landlords via the integrated mini-chat feature. The architecture uses Next.js for the user interface, API Gateway + Lambda for the backend, DynamoDB for main data storage, etc., for secure authentication. The key difference lies in the integration of OpenAI to automate tasks such as sending email/Telegram notifications when new, suitable rooms become available or generating statistical reports, increasing the system\u0026rsquo;s interactivity and reliability.\n2. Problem Statement Current Problem\nTraditional methods of finding accommodation (bulletin boards, social media groups) often lack detailed filtering systems, information is unfocused, and is easily disrupted by spam.\nCommunication between students and landlords is usually via phone/Zalo, creating a \u0026ldquo;friction\u0026rdquo; (communication barrier) and making it difficult to track history.\nLack of an automatic notification mechanism when new rooms become available that match students\u0026rsquo; specific needs.\nLandlords lack analytical tools regarding market demand and the number of views on listings to optimize rentals.\nSolution\nThe RoomMate platform provides a centralized rental search and management solution, integrating an internal mini-chat and automatic notifications (via n8n) based on individual criteria.\nAWS Serverless Architecture: Utilizes API Gateway + Lambda, DynamoDB (for room data, users, and chat), and S3 (for images) to ensure scalability at optimal cost (leveraging Free Tier).\nKey Features: Unlike generic chat/classifieds applications, RoomMate focuses on a positive tenant experience by using AI chatbots to suggest rental rooms based on individual requirements.\nBenefits and Return on Investment (ROI)\nBenefits: Provides a very practical solution for students. Reduces communication friction between parties through internal chat. Create a highly scalable platform (easy to add maps, review). Ensure all 5 core objectives of a technology project are met (serverless, CI/CD, monitoring, security, data pipeline).\nReturn on Investment (ROI): Extremely low development and operating costs due to maximizing the use of AWS Free Tier (Lambda, DynamoDB, S3), minimizing server management costs. The value delivered is a complete, practical product with outstanding features thanks to automation and internal chat capabilities.\n3. Solution Architecture The platform utilizes the AWS Serverless architecture to build a data management and analysis system. The system uses Amazon API Gateway and AWS Lambda to handle business logic. The web interface is distributed globally via CloudFront, acting as a single access point for both static content and API requests. Data is securely stored in Amazon S3 (for files/images) with protected buckets, while Amazon DynamoDB handles the storage of structured data. The deployment process is fully automated through AWS CodePipeline, sourced from GitHub. The entire operation is monitored by Amazon CloudWatch.\nAWS Services Used\nAWS Lambda: Executes business logic functions (Backend Business Logic).\nAmazon API Gateway: Receives, authenticates, and routes API requests from the web application to AWS Lambda.\nAmazon S3: S3 - Frontend stores static content of the web application (the destination of CodePipeline). S3 - Image Storage stores file and image data with protection mechanisms (S3 - Protected).\nAmazon DynamoDB: Stores structured data (NoSQL), including user data and device information.\nAWS CodePipeline: Automates CI/CD processes for the entire system, deploying the web interface to the S3 Frontend.\nAmazon Cognito: Manages user identities and access rights (authentication), protecting the API Gateway.\nAmazon CloudFront: Distributes web application content globally (from the S3 Frontend) and routes API requests to the API Gateway.\nAmazon CloudWatch: Monitors, logs, and provides alerts for the entire system.\nComponent Design\nUser Interface: The web application is hosted on the S3 Frontend and distributed globally via CloudFront.\nAPI Layer: The Lambda API Gateway handles business requests after users are authenticated by Cognito.\nData Storage: File and image data are stored in S3 Image Storage (B3 Image Storage). Structured data, users, and devices are stored in DynamoDB.\nDeployment Pipeline: AWS CodePipeline automates the build, test, and deployment of changes from GitHub to the S3 Frontend and resources.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Improve system quality through Backend Unit Tests and Frontend E2E Tests. Build a basic Data Pipeline \u0026amp; Business Intelligence (BI) system using AWS services. Finalize and optimize the CI/CD pipeline for both Frontend and Backend. Ensure system security by reviewing IAM permissions with the Least Privilege principle. Tasks for this Week: Day Tasks Start Date Completion Date References 2 - Backend Unit Tests using Jest: write unit tests for core Lambdas (roomCrud.js, searchRooms.js) 08/12/2025 08/12/2025 https://jestjs.io/ 3 - Frontend E2E Tests with Cypress: create test scenarios for 3 main flows: + Login + Search/Filter + Post Room 09/12/2025 09/12/2025 https://www.cypress.io/ 4 - Set up AWS Athena on exported data from S3 - Write athena-queries.sql: + Average rental price by District + Top 5 most viewed rooms 10/12/2025 10/12/2025 https://docs.aws.amazon.com/athena/ 5 - Configure Amazon QuickSight - Build a basic dashboard to visualize Athena query results 11/12/2025 11/12/2025 https://docs.aws.amazon.com/quicksight/ 6 - Optimize deploy-frontend.yml: Build Next.js → Upload to S3 → Invalidate CloudFront - Review IAM Permissions for all Lambdas (Least Privilege principle) 12/12/2025 12/12/2025 https://docs.github.com/en/actions Week 11 Achievements: Backend Unit Tests successfully implemented for: roomCrud.js searchRooms.js Frontend E2E Tests completed using Cypress for: Login Search \u0026amp; Filter Post Room System stability improved with minimum Test Coverage ~70%. Data Pipeline \u0026amp; BI successfully established: AWS Athena configured on S3-exported data. SQL queries created: Average rental price by district Top 5 most viewed rooms Amazon QuickSight dashboard created for data visualization. CI/CD pipeline fully completed: Automatic deployment for Frontend (Build → S3 → CloudFront) Backend pipeline reviewed and optimized. IAM permissions reviewed and restricted based on the Least Privilege principle. Final Result: The system now supports automated testing, business data analytics (BI), and fully automated CI/CD for both Frontend and Backend. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - Backup Amazon Elastic Kubernetes Service (EKS) resources using NetApp Trident Protect This blog post introduces the NetApp Trident Protect tool, a free solution designed for data protection, disaster recovery, and the migration of containerized workloads in the Amazon EKS environment. The article explains why EKS clusters require robust backup mechanisms to guard against human error, system failures, and region failures—risks that Kubernetes does not inherently protect against—including the backup of critical components like namespaces, Persistent Volumes, and configuration resources. Trident Protect allows users to perform on-demand or scheduled backups of Kubernetes resources to external storage backends such as Amazon S3, leveraging Kubernetes\u0026rsquo; native API and the tridentctl-protect CLI. Finally, the article provides a step-by-step guide for deploying a sample environment using Terraform, integrating Trident Protect with Amazon FSx for NetApp ONTAP to execute practical data protection and migration tasks.\nBlog 2 - AI-Driven Development Life Cycle: Reimagining Software Engineering This blog post introduces AI-Driven Development Life Cycle (AI-DLC), a new methodology proposed by AWS that reshapes software engineering by placing Artificial Intelligence (AI) at the center of the entire development lifecycle. AI-DLC is designed to address the limitations of current approaches (AI-assisted and AI-autonomous) by combining AI Execution with Human Supervision and Dynamic Team Collaboration. Accordingly, AI creates detailed work plans, seeks clarification, and executes solutions only after receiving human validation. The development process is divided into three phases: Inception (AI converts business intent into requirements), Construction (AI recommends architecture, models, code, and tests), and Operations (AI manages Infrastructure as Code and deployment). AI-DLC also replaces traditional “sprints” with “bolts”—shorter, more intense work cycles measured in hours or days. The core benefits of this approach are increased development velocity, innovation, quality, and improved developer experience, helping teams focus on problem solving instead of repetitive tasks.\nBlog 3 - Overcome development disarray with Amazon Q Developer CLI custom agents This blog introduces new Custom Agents in the Amazon Q Developer CLI, a feature designed to help developers efficiently manage the complexity and clutter of diverse development environments, especially in multi-tier applications. The core problem is that traditional AI assistants often have to guess context when using multiple tools (e.g., confusing a \u0026ldquo;table\u0026rdquo; in Figma with a \u0026ldquo;table\u0026rdquo; in PostgreSQL). Custom Agents allow developers to create specialized, optimized agents configured with specific tools, contexts, and permissions for each task (e.g., a separate Agent for the front-end and one for the back-end). By easily switching between Agents using the q chat \u0026ndash;agent [name] command, this feature reduces cognitive overhead, ensuring Amazon Q Developer provides more accurate and relevant answers, increasing productivity and quality of work.\nBlog 4 - How Zapier runs isolated tasks on AWS Lambda and upgrades functions at scale This blog explains how Zapier operates hundreds of thousands of isolated AWS Lambda functions to power user-created Zaps, ensuring strict tenant isolation, high scalability, and minimal operational overhead. By leveraging AWS Lambda’s Firecracker-based microVMs and an Amazon EKS–powered control plane, Zapier creates and manages functions dynamically for each Zap. A major challenge discussed is upgrading Lambda runtimes at scale as older runtimes reach end of life, requiring strong security compliance without disrupting customer experience. Zapier addressed this through a three-phase approach: cleaning unused functions with CloudWatch and Trusted Advisor, prioritizing critical and high-traffic functions for upgrades, and empowering engineering teams with Terraform modules and a custom Lambda runtime canary tool. This automated framework safely shifts traffic to upgraded versions, monitors errors, performs rollbacks if needed, and ultimately reduced outdated Lambda usage by 95%, ensuring secure, resilient, and continuously up-to-date serverless infrastructure.\nBlog 5 - How HashiCorp made cross-Region switchover seamless with Amazon Application Recovery Controller This blog describes how HashiCorp transformed its disaster recovery process by adopting Amazon Application Recovery Controller (ARC) to enable fast, reliable, cross-Region failover for its cloud platform. Previously dependent on complex, error-prone manual procedures, HashiCorp’s SRE team centralized the failover workflow using a custom orchestration service integrated with ARC’s globally available control plane. By redesigning DNS routing with Route 53 health checks, automating regional context signaling, and establishing rigorous disaster-recovery testing, HashiCorp now achieves seamless, consistent failover within minutes. ARC’s automation and resilience significantly improved recovery time, reduced operational risk, and strengthened the enterprise-grade reliability of HashiCorp Cloud Platform.\nBlog 6 - Implementing message prioritization with quorum queues on Amazon MQ for RabbitMQ This blog explains how to implement message prioritization when migrating from classic RabbitMQ queues—which have built-in priority support—to quorum queues on Amazon MQ for RabbitMQ, which do not support native priorities. It outlines why prioritization matters for time-critical business workflows and presents two main strategies: using separate quorum queues for each priority level or implementing custom consumer-side logic based on message metadata. These approaches allow organizations to maintain high availability and consistency while ensuring high-priority messages are processed first in quorum-based architectures.\nBlog 7 - Streamlining AWS Serverless workflows: From AWS Lambda orchestration to AWS Step Functions This blog discusses AWS Lambda as an orchestration delivery model and how to redesign serverless solutions using AWS Step Functions with unified primitives. Step Functions is a serverless service that you can use to build application delivery, automate workflows, orchestrate microservices, and create data streams and machine learning (ML). Step Functions provides primitives over 200 AWS services in addition to external APIs. You can use these properties to develop production-ready solutions with less effort, reduce coding complexity, improve long-term maintainability, and minimize technical debt when operating at larger scales.\nBlog 8 - Getting Started with Healthcare Data Lakes: Using Microservices This blog post highlights that as cloud adoption continues to accelerate, organizations are realizing that going to the cloud is just the beginning. The challenge – and the foundation for implementation – is using cloud optimization to deliver maximum business value. At AWS, we are committed to helping customers navigate this process successfully. Let’s explore some of the insights and best practices on the importance of cloud optimization shared in the new MIT Technology Review publication, Driving Business Value by Optimizing the Cloud.\nBlog 9 - Introducing v2 of Powertools for AWS Lambda (Java) This blog introduces the new version of Powertools for AWS Lambda (Java) with support for GraalVM native images. You\u0026rsquo;ll learn how GraalVM reduces cold starts by up to 75%, how modular design reduces memory usage, and the benefits of core utilities like logging, metrics, and tracing. The article also covers migration steps from version 1, configuring native compilation, and ensuring compliance with serverless development standards.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"{\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Co., Ltd. from September 29, 2025 to November 22, 2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at school to a real-world work environment.\nI participated in frontend development and backend testing, thereby improving my skills in programming, data analysis, report writing, and communication.\nRegarding my work ethic, I always strived to complete tasks well, adhered to company regulations, and actively collaborated with colleagues to improve work efficiency.\nTo objectively reflect my internship experience, I would like to self-assess myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/7-feedback/","title":"Sharing, Contributing Opinions","tags":[],"description":"","content":"Overall Assessment 1. Working Environment Throughout my internship, I clearly felt a friendly, open, and positive working environment. The FCJ team members were always ready to support me when I encountered difficulties, even outside of working hours. The workspace was neat and comfortable, helping me to focus better each day. If there were improvements, I hope the company could organize more team-bonding activities to further strengthen everyone\u0026rsquo;s bonds.\n2. Support from Mentors/Team Admins Regarding mentors and team admins, I am truly grateful for their dedication and enthusiasm. The mentors provided clear guidance, patiently explained things when I was confused, and always encouraged me to proactively ask questions. What I appreciate most is that my mentor didn\u0026rsquo;t \u0026ldquo;do it for me,\u0026rdquo; but instead gave me the opportunity to think for myself and solve problems. The admin team also provided excellent support with materials, procedures, and created a favorable environment for me to focus on my studies and work.\n3. Job and Major Match The assigned work greatly supported my major, helping me reinforce my foundational knowledge from school while also opening up many new areas of knowledge that I hadn\u0026rsquo;t encountered before. As a result, I not only improved my theoretical knowledge but also gained a clearer understanding of how that knowledge is applied in practice.\n4. Opportunities for Learning \u0026amp; Skill Development During the internship, I had the opportunity to learn and develop many important skills such as teamwork, using project management tools, and communication in a professional work environment. In addition, the practical insights from my mentor gave me a clearer vision of my future career path.\n5. Culture \u0026amp; Team Spirit The culture and team spirit are a major highlight. Everyone respects each other, works seriously but without stress or pressure. When there\u0026rsquo;s urgent work, everyone is ready to support each other regardless of position. Thanks to this, even as an intern, I always feel like I\u0026rsquo;m part of a team.\n6. Policies/Benefits for Interns Regarding policies for interns, the company provides allowances and flexible working hours when needed. In particular, participating in internal training sessions helped me learn a lot of knowledge and practical experience.\nOther Questions What were you most satisfied with during your internship?\nWhat were you dissatisfied with during your internship?\nWhat do you think the company needs to improve to better support interns?\nHow would you rate the work environment at the company?\nHow did you feel about the support from your mentor throughout your internship?\nWas the assigned work relevant to your major?\nWhat were the most important skills you learned during your internship?\nDid you encounter any difficulties during your work? If so, what were they?\nHow would you rate the workload?\nDid you feel pressured during your internship? Why?\nIf you were to recommend this company to friends, would you advise them to intern here? Why?\nWhat would make you willing to recommend (or not recommend) the company to others?\nSuggestions \u0026amp; Wishes I hope that in the future the company can organize more career experience sharing sessions and development orientations to help interns better understand their future paths. If given the opportunity, I would very much like to continue participating in this program in the future, not only as an intern but also in more permanent roles.\n"},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nguyenmaivy.github.io/nguyenmaivy/tags/","title":"Tags","tags":[],"description":"","content":""}]